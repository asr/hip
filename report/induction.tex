\section{Structural induction}
\label{sec:induction}

Properties that are true merely by rewriting the definitions are
neither abundant nor especially interesting. The fundamental concept
of constructors and pattern matching in Haskell is closely related to
induction. The values of an argument with a concrete type can only
range over the data type's different constructors. Such a case
analysis combined with induction is especially strong.

Induction is a very fundamental concept of mathematics and is directly
or indirectly an axiom in axiomatisation of mathematics. We shall
take the Peano Arithmetic, $\PA$, axioms for true. This theory has the
natural numbers as standard model with a small vocabulary consisting
only of the constant $0$, the unary successor function $s$, and binary
plus and multiplication. Induction in $\PA$ allows proving properties
that hold for all natural numbers. The first proof obligation in a
proof by induction is to prove that the property holds for zero. The
second is if it holds for an arbitrary but fixed number then it must
hold for its successor. The induction schema is formally written as
this:

\begin{mathpar}
  \inferrule* % [Left=PA-Ind]
     {
       \overbrace{P(0)}^{\mathrm{base}}
       \\
       \overbrace{
           \fa{x}
                 \underbrace{P(x)}_{\mathrm{hypothesis}}
              \rightarrow
                 \underbrace{P(s(x))}_{\mathrm{conclusion}}
       }^{\mathrm{step}}
     }
     { \fa{x} P(x) }
\end{mathpar}

Different parts of this rule has been highlighted that are common
names for induction proofs: the induction base, the induction step
with the assumed hypothesis and the obligated conclusion.
%The conclusion is sometimes called the \emph{goal} and the hypothesis the \emph{given}.
This is a axiom \emph{schema} since it is not possible to quantify
over a predicate in first order logic. Rather, it is a infinite set of
axioms, one for each (well-formed) formula instantiated in place for
$P$. Generally, theorem provers do not instantiate schemata themselves
so it has to be done manually, with an appropriate formula for $P$. In
this thesis, the predicates will be equalities that depend on the
argument to $P$. For example, to show that two functions $f$ and $g$
are equal, $P(x)$ is defined $P(x)$ to be $f(x)=g(x)$.

To prove a property with induction in Haskell, induction can be
carried out on the number of constructors in a value. It is possible
to get many base cases, one for each non-recursive constructor and
then a step case for each recursive constructor. As an example, we can
consider one of the simplest recursive data structure in Haskell, the
Peano Natural numbers in Haskell, defined as
\hs{data Nat = Zero | Succ Nat}. This axiom schema is used and
follows the same structure as in $\PA$:

\begin{mathpar}
  \inferrule*
     {
       P(\hs{Zero})
       \\
       \fa{x} P(x) \rightarrow P(\hs{Succ} \w x)
     }
     { \fa{x} x \text{ finite and total} \rightarrow P(x) }
\end{mathpar}

Any data type gives rise to induction schemata, and this is called
structural induction. These are the axiom schemata for lists and the
\hs{Tree} type defined in Section \ref{sec:treetrans}:

\begin{mathpar}
  \inferrule* % [left=List-Ind]
     {
       P(\hs{[]})
       \\
       \faa{x}{xs} P(xs) \rightarrow P(x\hs{:}xs)
     }
     { \fa{xs} xs \text{ finite and total} \rightarrow P(xs) }

  \inferrule* % [left=Tree-Ind]
     {
       P(\hs{Empty})
       \\
       \faaa{l}{x}{r} P(l) \wedge P(r) \rightarrow P(\hs{Fork} \w l \w x \w r)
     }
     { \fa{t} t \text{ finite and total} \rightarrow P(t) }
\end{mathpar}

For indexed sum of products data types the translation is
straightforward: for a data type $T$, with a constructor $K$ you get
all recursive arguments of $T$ to $K$ as hypotheses.

\subsection{Partial and Infinite Values}
\label{sec:admissible}

It is also possible to use induction to prove properties about all
infinite and partial values of a data types. The predicate must then
be \emph{admissible}, and is formally defined below. Informally, it
means that the predicate preserves $\omega$-chains of
$\sqsubseteq$. Such chains are described in more detail in Section
\ref{sec:domaintheory}, an example of such a is:

\begin{equation*}
\bot \sqsubseteq
\hs{Succ} \w \bot \sqsubseteq
\hs{Succ} \w (\hs{Succ} \w \bot) \sqsubseteq
\hs{Succ} \w (\hs{Succ} \w (\hs{Succ} \w \bot)) \sqsubseteq
\cdots
\end{equation*}

This chain has the limit \hs{inf}, defined as \hs{inf = Succ inf}. If
$P$ is an admissible predicate, then if $P$ holds for every element in
that chain then $P(\hs{inf})$. We can now define admissible.

\paragraph{Definition} A predicate $P$ is \emph{admissible} for every
$\omega$-chain $\langle x_n \rangle_{n \in \omega}$}, this holds:

\begin{equation*}
(\fa{n} P(x_n)) \Rightarrow P(\lub{n \in \omega}(x_n))
\end{equation*}

The properties concerned about in this thesis are about equality, and
equality of continuous functions is can be proved to be
admissible. Assume two such functions $f$ and $g$, and define the
predicate as $P(x) :\Leftrightarrow f(x) = g(x)$. Take any chain
$\langle x_n \rangle_{n \in \omega}$, and assume $P$ holds for every
element in the chain, then:

\begin{align*}
\desca{assumption}               \\
& \fa{n} P(x_n)                    \\
\desclra{definition}             \\
& \fa{n} f(x_n) = g(x_n)           \\
\descra{limits}                  \\
& \lub{n}(f(x_n)) = \lub{n}(g(x_n))  \\
\desclra{continuity}             \\
& f(\lub{n}(x_n)) = g(\lub{n}(x_n))  \\
\desclra{definition}             \\
& P(\lub{n}(x_n))
\end{align*}

This result generalises to equalities of compositions of continuous
functions. One way to show this is to use that the continuous
functions over complete partial orders form a category with products
and exponentials. These are the necessary components of composition
and application.

To conclude: if you want to show that an admissible property holds for
partial and infinite values you also need to consider $\bot$ as a
constructor for the data type. An example of this is the induction
principle for possibly partial and infinite lists:

\begin{mathpar}
  \inferrule* % [left=List-Ind]
     {
       P(\bot)
       \\
       P(\hs{[]})
       \\
       \faa{x}{xs} P(xs) \rightarrow P(x\hs{:}xs)
       \\
       P \text{ admissible}
     }
     { \fa{xs} P(xs) }
\end{mathpar}


\subsection{Generalisations}
\label{sec:genind}

The structural induction schemata seen so far only uses each
constructor once and that does not work for proving properties about
functions defined with recursion with a bigger depth. For instance,
the induction on Peano numbers in Haskell can be adjusted to a
constructor depth two in this way:

\begin{mathpar}
  \inferrule* % [left=Nat-Ind-Depth-Two]
     {
       P(\bot)
       \and
       P(\hs{Zero})
       \and
       P(\hs{Succ Zero})
       \\
       \fa{x} P(x) \wedge P(\hs{Succ} \w x) \rightarrow P(\hs{Succ} \w (\hs{Succ} \w x))
     }
     { \fa{x} P(x) }
\end{mathpar}

Given how many constructors you maximally want to unroll for your data
type, you have to prove the property for all combinations of
constructors up to including that limit, but for an induction step
with a conclusion with $i$ constructors, the induction hypothesis is
the conjunction of all combinations with strictly less than $i$
constructors.

%\pagebreak

\begin{comment}
Say you want to prove a property that has a list of natural numbers as
property, with up to two constructors, these are the required
induction steps:

\begin{align*}
\rom{1} &&                                                                & P(\hs{[]}) \\
\rom{2} &&                                                                & P(\hs{Zero}\hs{:}\hs{[]}) \\
\rom{3} && \fa{x} P(x\hs{:}\hs{[]}) \rightarrow                        \, & P(\hs{Succ} \w x\hs{:}\hs{[]}) \\
\rom{4} && \faaa{x}{y}{zs} P(x\hs{:}zs) \wedge P(y\hs{:}zs)\rightarrow \, & P(x\hs{:}y\hs{:}zs) \\
\end{align*}

\end{comment}

\pagebreak

This is used to use induction on more than one arguments, by using the
tuple constructor. The base and the step cases of induction on two
natural numbers is this:

\begin{align*}
\rom{1} &&                                                                                        & P(\hs{Zero}     ,\hs{Zero}     ) \\
\rom{2} && \fa{x} P(x,\hs{Zero})                                                   \rightarrow \, & P(\hs{Succ} \w x,\hs{Zero}     ) \\
\rom{3} && \fa{y} P(\hs{Zero},y)                                                   \rightarrow \, & P(\hs{Zero}     ,\hs{Succ} \w y) \\
\rom{4} && \faa{x}{y} P(x,y) \wedge P(\hs{Succ} \w x,y) \wedge P(x,\hs{Succ} \w y) \rightarrow \, & P(\hs{Succ} \w x,\hs{Succ} \w y) \\
\end{align*}

This can be used to show the commutativity of plus for natural numbers
defined recursively on the first argument. The predicate is defined as
$P(x,y) \Leftrightarrow \faa{x}{y} x + y = y + x$. Only these two
lemmas are needed if you do this with induction on the first argument:
\begin{enumerate}
  \item $\fa{x} x + \hs{Zero} = x$. This corresponds the hypothesis
    $P(x,\hs{Zero})$ above.
  \item $\faa{x}{y} \hs{Succ} \w x + y = x + \hs{Succ} \w y$.
    Derivable from two of the hypotheses in axiom $\romnodot{4}$:
    \begin{align*}
    lhs & = \hs{Succ} \w x + y   && \{\textrm{definition of $+$}\} \\
        & = \hs{Succ} \w (x + y) && \{\textrm{hypothesis $P(x,y)$}\} \\
        & = \hs{Succ} \w (y + x) && \{\textrm{definition of $+$}\} \\
        & = \hs{Succ} \w y + x   && \{\textrm{hypothesis $P(x,\hs{Succ} \w y)$}\} \\
        & = x + \hs{Succ} \w y = rhs
    \end{align*}
\end{enumerate}

The commutativity of plus does only hold for total values since we have
$$\bot \eq \bot + \hs{Succ Zero} \, \neq \, \hs{Succ Zero} + \bot \eq \hs{Succ} \w \bot$$

Unrolling data types in this way to construct hypotheses also work
well with data types defined in terms of other data types. An example
is the definition of $\mathbb{Z}$ defined with a disjoint union as
$\mathbb{N} + \mathbb{N}$, which can be defined as this in Haskell:

\begin{code}
data Z = Pos Nat | Neg Nat
\end{code}

A value of $\hs{Pos} \w x$ denotes the integer $+\, x$ and $\hs{Neg} \w y$
the integer $-(1+y)$. This avoids having two values denoting
$0$. With the above technique, the induction principle for total
\hs{Z} with two constructors is:

\begin{mathpar}
  \inferrule* % [left=Nat-Ind-Depth-Two]
     {
       P(\hs{Pos Zero})
       \and
       \fa{x} P(\hs{Pos} \w x) \wedge P(\hs{Neg} \w x) \rightarrow P(\hs{Pos} \w (\hs{Succ} \w x))
       \\
       P(\hs{Neg Zero})
       \and
       \fa{x} P(\hs{Pos} \w x) \wedge P(\hs{Neg} \w x) \rightarrow P(\hs{Neg} \w (\hs{Succ} \w x))
     }
     { \fa{x} P(x) }
\end{mathpar}

\subsection{Skolemised Hypotheses}

Each induction step can be expressed with skolemised variables instead
of universally quantified variables. This means that a new constant is
introduced for each variable. They can then be referred to in
different axioms in the theory. For example, when adding the
\hs{Branch} step for an induction proof over the \hs{Tree} data type,
the theorem prover could be run like this:

\begin{equation*}
T \vdash \faaa{l}{x}{r} P(l) \wedge P(r) \rightarrow P(\fn{branch}(l,x,r))
\end{equation*}

Skolemisation of each universally quantified variable allows to
instead prove this equivalent judgement:

\begin{equation*}
T , P(l) , P(r) \vdash P(\fn{branch}(l,x,r))
\end{equation*}

This prevents long formulas should there be many hypotheses.  It also
makes the implementation of the simple induction technique with one
argument and one constructor straightforward. For each constructor $C$
with $n$ arguments, we make a new call to a theorem prover with $n$
Skolem variables $x_1 \cdots x_n$ and try to prove
$P(x_1,\cdots,x_n)$.  Additionally, for each argument with assigned
variable $x_i$, if the type of this variable is the same as the
constructor $C$'s, $P(x_i)$ is added as an axiom to the theory: this
an induction hypothesis.

The predicate is inlined: there is no introduction of $P$ and its
definition, it is replaced with the property. Suppose we want to prove
the property
$\fa{t} \fn{mirror}(\fn{mirror}(t)) = t$ where $t : \hs{Tree a}$,
the \hs{Branch} case generates this call to the theorem prover:

\begin{align*}
\mathcal{T} \, & , \, \fn{mirror} \w (\fn{mirror} \w x_1) \eq x_1 \\
            \, & , \, \fn{mirror} \w (\fn{mirror} \w x_3) \eq x_3 \\
               &\vdash \fn{mirror} \w (\fn{mirror} \w (\fn{branch} \w x_1 \w x_2\w x_3) \eq \fn{branch} \w x_1 \w x_2\w x_3
\end{align*}

Here $\mathcal{T}$ is the theory for this Haskell program: it includes
the definition of $\hs{mirror}$, along with axioms that $\hs{Branch}$,
$\hs{Empty}$ and $\bot$ are disjoint.  Hypotheses for $x_1$ and $x_3$
are added since the first and the third argument for the \hs{Branch}
constructor is \hs{Tree a}. For $x_2$, nothing is added since the type
of this is just \hs{a}.  For this property, in total three invocations
to a theorem prover is carried out: one for each constructor, and one
for bottom.

\begin{comment}
    \note{I don't elaborate on this too much since the current
      implementation generates tree of a given depth instead of a given
      number of constructors. Specifying constructors could potentially
      give you less granularity: I need to investigate this}
    To generate theories for the approach in \ref{sec:genind} an algorithm
    was written to generate trees with a number of constructors. For the
    commutativity of plus example, you would start with the expression
    \hs{(x :: Nat,y :: Nat)}, and for each occurrence of a typed variable,
    you replace it with its constructors, and combine. Then an other
    algorithm takes a typed expression, like
    \hs{(Succ (x :: Nat),Succ (y :: Nat))} and returns all expressions
    with less constructors you can generate with the correct type with
    the typed variables, here \hs{(Nat,Nat)}. Then you proceed as in the
    simple case: one invocation to the theorem prover for each step,
    variables are skolemised and the predicate inlined.

    This leads to a combinatorial explosion for a lot of data types. If
    you need to prove something with length $n$ lists, you get a induction
    conclusion $P(x_1\hs{:}x_2\hs{:}\cdots\hs{:}x_n\hs{:}xs)}$, and as there
    are $n!$ combinations of $x_1 \cdots x_n$, making this approach
    unusable for high $n$. A better way would probably to just give you
    all sub trees instead of all possible trees: again, this of course
    depends on what you need to show.
\end{comment}

\subsection{Future work}
\label{sec:futind}

\paragraph{Exponential Types} It is also possible to use structural
induction for exponential data types. Examples are higher-order
abstract syntax, another is the Brouwer ordinals defined by
\citet{dixonphd} as:

\begin{code}
data Ord = Zero | Succ Ord | Lim (Nat -> Ord)
\end{code}

The induction schema for this data type is:

\begin{mathpar}
  \inferrule* % [left=Ord-Ind]
     {
       P(\bot)
       \\
       P(\hs{Zero})
       \\
       \fa{x} P(x) \rightarrow P(\hs{Succ} \w x)
       \\
       \fa{f} (\fa{x} P(f \w x)) \rightarrow P(\hs{Lim} \w f)
     }
     { \fa{x} P(x) }
\end{mathpar}

This was never implemented since the lack of examples with exponential
data types.

\paragraph{Lexicographic Induction} In the rest of this section only
induction in $\PA$ is discussed. Again, $\PA$ has $0$ as zero and the
successor function $s$. If you need induction on two variables, to
show $\faa{x}{y} P(x,y)$, an other way than generalised above is to
use lexicographic induction. First you do induction on $x$ to get the
proof obligations $\fa{y} P(0,y)$ and $(\fa{y} P(x,y)) \rightarrow
(\fa{y} P(s(x),y))$.  Notice that in the step case, $y$ is universally
quantified in both the hypothesis and in the conclusion. In both the
base and in the step you can now do induction on $y$ to get this
inference rule:

\begin{mathpar}
  \inferrule* % [left=Nat-Ind-Depth-Two]
     {
       P(0,0)
       \and
       \inferrule{P(0,y)}{P(0,s(y))}
       \and
       \inferrule{\fa{y'} P(x,y')}
                 {P(s(x),0)}
       \and
       \inferrule{\fa{y'} P(x,y') \and P(s(x),y)}
                 {P(s(x),s(y))}
     }
     { \faa{x}{y} P(x,y) }
\end{mathpar}

This is not yet implemented since it was not clear which examples
would benefit from it, and the many combinations of how to apply this
principle: in the example above induction is first on $x$, and then on
$y$, but the other way around is also legit. If a property has $n$
variables that could be used for induction, and you use all variables,
you have $n!$ ways of doing lexicographic induction, and this is
disregarding subsets of variables.  A solution to this problem, as
well as the problem with how many constructors you want to use in the
generalised induction, is to allow the user to annotate in the source
code the desired way to do induction: furthermore, it does not blend
well with the rest being a fully automated verification of properties.


\paragraph{Automated Depth}
Another way to do this is to encode different induction ways by means
of axioms in the theory itself. A simple example of this is to prove a
property $P$ on one Peano natural number, and choose how many base
cases in form of $P(0)$, $P(1)$, up to $P(n)$ to get a stronger
induction hypothesis $P(x)\wedge P(x+1)\wedge \cdots\wedge
P(x+n-1)\rightarrow P(x+n)$. It is possible to axiomatise such a
``machine'' in first order logic as this:

\newcommand{\QZ}[2]{\inferrule* [left=q-0]{ }{Q(#1,#2)}}
\newcommand{\QS}[3]{\inferrule* [left=q-s]{P(#2) \and #3}{Q(#1,#2)}}
\newcommand{\IPZ}[2]{\inferrule* [left=+-0]{#1}{#2}}
\newcommand{\IPS}[2]{\inferrule* [left=+-s]{#1}{#2}}

\begin{mathpar}
  \QZ{0}{x}
  \and
  \QS{s(n)}{x}{Q(n,s(x))}
  \and
  \IPZ{ }{0 + x=x}
  \and
  \IPS{ }{s(n) + x=s(n+x)}
  \\
  \inferrule*{\ex{n} Q(n,0) \wedge
                     \fa{x} Q(n,x) \rightarrow P(n+x)}
             {\fa{x} P(x)}
\end{mathpar}

The $n$ in the existential quantifier is how deep the induction needs
to go. $Q$ is a predicate which both gives a suitable induction base
and hypothesis for $n$. For the base case, prove $P(0)$, $P(1)$, up to
$P(n-1)$. In the induction step,$P(x)$, $P(x)$, up to $P(x+n-1)$ are
hypotheses. The consequent is $P(n+x)$, so some axioms for $+$ are
borrowed from $\PA$. The degenerate case come from $n=0$:

\begin{mathpar}
  \inferrule*{
     \QZ{0}{0}
     \and
     \fa{x}      \QZ{0}{x}
     \rightarrow \IPZ{P(x)}{P(0+x)}
    }
    {\fa{x} P(x)}
\end{mathpar}

\pagebreak
A little bit of expanding is necessary for $n=2$ to obtain:

\begin{mathpar}
    \inferrule*
      {
        \QS{2}{0}{\QS{1}{1}{\QZ{0}{2}}}
       \\
       \fa{x}      \QS{2}{x}{\QS{1}{s(x)}{\QZ{0}{s(s(x))}}}
       \rightarrow \IPS{
                        \IPS{
                              \IPZ{P(s(s(x)))}
                                  {P(s(s(x+0)))}
                            }
                            {P(1+s(x))}
                       }
                       {P(2+x)}
      }
      {
        \inferrule*
          {P(0) \and P(1) \and \fa{x} P(x) \wedge P(s(x)) \rightarrow P(s(s(x)))}
          {\fa{x} P(x)}
      }
\end{mathpar}

The complexity is greatly increased with more irregular data types
than natural numbers, and it is unclear how well theorem provers would
be able to handle this.

