\chapter{Haskell to First Order Logic}
\label{ch:translation}

To enable automated theorem provers to do equational reasoning of
Haskell programs a translation to first order logic is needed. It is
here referred to as a translation, but it could also be regarded as a
compilation. The idea is to use constants and functions in first order
logic to correspond to constructors and functions, and arguments to
functions need to be universally quantified. We shall try to do a
na\"{\i}ve attempt of a translation with this ideas and see how far it
takes us.

\section{Na\"{\i}ve Translation}
\label{sec:treetrans}

We will use a data type of binary trees with an element at every
branch, and consider some examples of functions defined on it. The
definition of the data type is:

\begin{code}
data Tree a = Fork (Tree a) a (Tree a) | Leaf
\end{code}

\noindent
With the idea above, occurrences of the \hs{Fork} constructor in the
source code should be translated to a logic function $\fn{fork}$, and
similarly a constant for \hs{Leaf}. How should we then translate the
\hs{singleton} function, defined below?

\begin{code}
singleton :: a -> Tree a
singleton x = Fork Leaf x Leaf
\end{code}

\noindent
Following our intuition we make an universal quantification for
\hs{x}, and a new logic function for \hs{singleton}. The result
is this axiom:
\begin{equation*}
\fa{x} \fn{singleton}(x) = \fn{fork}(\fn{leaf},x,\fn{leaf})
\end{equation*}

\noindent
So far so good, but what if someone wants to prove that \hs{singleton
  x} is a \hs{Leaf}? With a theory with only this axiom there are
models with only one element where \hs{Leaf} is equal to
\hs{Fork}. Axioms needs to be added expressing that values created
from different constructors are unequal. We will call those
\emph{disjoint constructor axioms}. The sole axiom for \hs{Tree} is:
\begin{equation*}
\faaa{l}{x}{r} \fn{leaf} \neq \fn{fork}(l,x,r)
\end{equation*}

Constructors should also be injective to get regular models, and
expressing such axioms is straightforward. For \hs{Tree} only \hs{Fork} has
arguments and this injectivity axiom is needed:
\begin{equation*}
\faaaaaa{l_0}{l_1}{x_0}{x_1}{r_0}{r_1} \fn{fork}(l_0,x_0,r_0) \eq
\fn{fork}(l_1,x_1,r_1) \rightarrow l_0 \eq l_1 \wedge x_0 \eq x_1 \wedge r_0 \eq r_1
\end{equation*}

To describe pattern matching, we consider a \hs{mirror} function,
which recursively swaps the left sub tree with the right and
vice-versa, we follow our intuition to translate the pattern matching
and to these two axioms\footnote {Axioms are enumerated by Roman
  numerals to tell them apart.}:

\begin{code}
mirror :: Tree a -> Tree a
mirror (Fork l x r) = Fork (mirror r) x (mirror l)
mirror Leaf         = Leaf
\end{code}
\begin{align*}
\rom{1} && \faaa{l}{x}{r} & \fn{mirror}(\fn{fork}(l,x,r)) \eq \fn{fork}(\fn{mirror}(r),x,\fn{mirror}(l)) \\
\rom{2} &&                & \fn{mirror}(\fn{leaf}) \eq \fn{leaf}
\end{align*}

\noindent
A problem with this translation is that there are no axioms for other
arguments of $\fn{mirror}$ than leafs and forks, and we have models that
include other values than leafs and forks. Another problem is
encountered for \hs{singleton}'s left inverse \hs{top} defined below,
which returns the top element of a \hs{Tree}. This is a partial
function since the \hs{Leaf} pattern is omitted:

\begin{code}
top :: Tree a -> a
top (Fork l x r) = x
\end{code}

The translation must capture the pattern match failure that results
from trying to evaluate \hs{top} applied to a \hs{Leaf}. We conclude
that this na\"{\i}ve translation does not take us further, but we
shall see in the next section how to fix this.

\section{Bottom and Pattern Matching}

We borrow the concepted bottom from domain theory. It is denoted
$\bot$ and is the least defined value: pattern match failures, use of
\hs{error} and \hs{undefined} in the source code, and also
non-terminating functions. For \hs{top} the idea is to add an axiom so
that $\fn{top}$ of anything that is not a \hs{Fork} is bottom. This is
an example of such an axiomatization:

\begin{align*}
\rom{1} \qquad & \faaa{l}{x}{r} \fn{top}(\fn{fork}(l,x,r)) \eq x \\
\rom{2} \qquad & \fa{t}         (\nexxx{l}{x}{r} \fn{fork}(l,x,r)) \eq t) \rightarrow \fn{top}(t) \eq \bot
\end{align*}

Most theorem provers would as a preprocessing step \note{\qquad \qquad citation
  needed}skolemize the existential quantification in the second
axiom. A new unary function would be introduced for $l$, $x$ and $r$,
depending on $t$, an arbitrary choice of names are $\fn{top}$ prependend
to the original variable. The axiom then looks like
this\footnote{This thesis uses the same convention for quantifiers as
  for lambda functions: they bind as far as possible.}:
\begin{align*}
\rom{2}' \qquad & \fa{t} \fn{fork}(\fn{topl}(t),\fn{topx}(t),\fn{topr}(t))) \neq t \rightarrow \fn{top}(t) \eq \bot
\end{align*}

For another function, like \hs{mirror} above, one of the skolemized
functions could be called $\fn{mirrorl}$. Since axioms of injective
constructors are also added, a theorem prover could, in some steps,
conclude that $\fn{mirrorl}(\fn{fork}(l,x,r)) \eq
\fn{topl}(\fn{fork}(l,x,r)) \eq l$. Instead such skolemized
\emph{selection functions} are introduced manually.  For the \hs{Fork}
constructor let us call them $\fn{fork_0}$, $\fn{fork_1}$ and
$\fn{fork_2}$, and their axioms are:
\begin{align*}
\rom{1} \qquad \faaa{l}{x}{r} \fn{fork_{0}}(\fn{fork}(l,x,r)) & \eq l \\
\rom{2} \qquad \faaa{l}{x}{r} \fn{fork_{1}}(\fn{fork}(l,x,r)) & \eq x \\
\rom{3} \qquad \faaa{l}{x}{r} \fn{fork_{2}}(\fn{fork}(l,x,r)) & \eq r
\end{align*}

\noindent
The translation of \hs{top} with these selector functions is:
\begin{align*}
\rom{1} \qquad & \faaa{l}{x}{r} \fn{top}(\fn{fork}(l,x,r)) \eq x \\
\rom{2} \qquad & \fa{t}         (\fn{fork}(\fn{fork_0}(t),\fn{fork_1}(t),\fn{fork_2}(t))) \neq t) \rightarrow \fn{top}(t) \eq \bot
\end{align*}

\noindent
Another nice side effect of writing in this skolemized selector style
is that implies injective constructors. Assume we have
$\fn{fork}(l_0,x_0,r_0)=\fn{fork}(l_1,x_1,r_1)$ then the first
selector, $\fn{fork_0}$, gives $l_0=l_1$. Analogously the second and
the third give $x_0=x_1$ and $r_0=r_1$, respectively. Thus selector
axioms are added in place of of injectivity axioms.

With the bottom constant in the theory, the axioms disjointedness are
effected by this. It can be seen as an implicit constructor for every
data type. For the \hs{Tree} data type the axioms are:

\begin{align*}
\rom{1} \qquad & \faaa{l}{x}{r} \fn{fork}(l,x,r) \neq \fn{leaf} \\
\rom{2} \qquad & \faaa{l}{x}{r} \fn{fork}(l,x,r) \neq \bot      \\
\rom{3} \qquad & \bot \neq \fn{leaf}
\end{align*}

Now we have a good idea how to translate pattern matching, but
in Haskell we can pattern match almost everywhere! How would we
proceed to translate a function like this, taken from the
implementation of \hs{scanr} from the \hs{Prelude}?

\begin{code}
scanr             :: (a -> b -> b) -> b -> [a] -> [b]
scanr f q0 []     =  [q0]
scanr f q0 (x:xs) =  f x q : qs
                     where qs = scanr f q0 xs
                           q = case qs of
                                 q : _ -> q
\end{code}

\noindent
There is both pattern matching directly on the arguments, but also
pattern matching in a case statements in the where function
\hs{q}. There can also be pattern matching in lambdas. To help with
these difficulties, we define an intermediate language in the next
section.

\section{The Intermediate Language}

To address the difficulties of pattern matching elsewhere than in the
arguments of a function, a small intermediate language was designed
that can only do pattern matching at a very controlled location: in a
case statement that is the entire body of a function, and all arms are
simple expressions consisting of function and constructor applications
and variables. We translate Haskell is translated to language. Pattern
matching matching at other locations are moved to new top level
definitions. Functions definined in let and where are raised to the
top level, with the necessary variables in scope as additional
arguments. The same is done for sections and lambda functions. The BNF
for the language is this:

\begin{equation*}
\begin{aligned}
\text{Variables} \quad & x \\
\text{Functions} \quad & f \\
\text{Constructors} \quad & C \\
\text{Type variables} \quad & \tau \\
\text{Type constructors} \quad & T \\
\defBNF{Declarations}{decl}{ f \; \overline{x} \; \hs{=} \; body}{function declaration} \\
    \defaltBNF{f \; :: \; t}{type signature} \\
    \defaltBNF{\hs{data} \; T \; \overline{\tau} \; \hs{=} \; \overline{C \; \overline{t}}}{data type declaration} \\
\defBNF{Function body}{body}{\hs{case} \; e \; \hs{of} \; \overline{alt}}{case body} \\
    \defaltBNF{e}{expression body} \\
\defBNF{Expressions}{e}{x}{variable} \\
    \defaltBNF{f \; \overline{e}}{function application} \\
    \defaltBNF{C \; \overline{e}}{constructor application} \\
\defBNF{Alternative}{alt}{pat \rightarrow e}{branch without guard} \\
    \defaltBNF{pat \; \hs{|} \; e \rightarrow e}{branch with guard} \\
\defBNF{Pattern}{p}{x}{pattern variable} \\
    \defaltBNF{C \; \overline{p}}{constructor pattern} \\
\defBNF{Types}{t}{\tau}{type variable} \\
    \defaltBNF{t \; \rightarrow \; t}{function type} \\
    \defaltBNF{T \; \overline{\tau}}{type constructor application} \\
\defBNF{Programs}{prog}{\overline{decl}}{} \\
\end{aligned}
\end{equation*}

This language is a strict subset of Haskell. Repeated entities are
notated with an $\overline{\text{overline}}$.  Data declarations are
needed to generate axioms of disjointedness and selectors. Type
signatures are ignored in the translation, but the proof techniques
introduced later use this information.

A function is just a function name with a number of variables, and
then a function body, which is either an expression of variables,
functions and constructors, or a case statement with an expression
scrutinee. Branches consists of a pattern, possibly with nested uses
of constructors, and an optional guard, and in the arm is an
expression.

Now we need to distinguish between two translations: the intermediate
translation from Haskell to the intermediate language, and the logic
translation from this language to first order logic.

\section{The Intermediate Translation}

This section describes the transformation from Haskell to the
intermediate language. The main transformations are top level lifting
of lambdas, local definitions and restricting pattern matching only in
case statements.

\paragraph{Argument pattern matching} A function that does pattern matching will be translated to one that
takes in unmatched arguments and with a case in the body. The
\hs{mirror} function above is thus translated to this:

\begin{code}
mirror :: Tree a -> Tree a
mirror t = case t of
   Fork l x r -> Fork (mirror r) x (mirror l)
   Leaf       -> Leaf
\end{code}

\noindent
If you do pattern matching on several arguments, the scrutinee in the
case will be a tuple of all the arguments.

\paragraph{Local definitions} Where-clauses and let-expressions are
raised to the top level, with all necessary variables as
arguments. This example of an accumulator definition of multiplication
of Peano natural numbers needs such a rewrite:

\begin{code}
(*) :: Nat -> Nat -> Nat
x * y = go Zero x
  where
    go acc Zero    = acc
    go acc (Suc n) = go (acc + y) n
\end{code}

\noindent
The \hs{go} function has the \hs{y} in scope but not as argument so it
is appended to the arguments to the top level lifted version of \hs{go}:

\begin{code}
go acc Zero    y = acc
go acc (Suc n) y = go (acc + y) n y

x * y = go Zero x y
\end{code}

\noindent
Finally the pattern matching in \hs{go} is translated to use a case expression:

\begin{code}
go acc x y = case x of
     Zero  -> acc
     Suc n -> go (acc + y) n y
\end{code}

A similar translation is done for let expressions.

\paragraph{Lambda functions} These are translated to top level
definitions. Take this example of defining \hs{fmap} in terms of the
functions from the \hs{Monad} type class:

\begin{code}
liftM :: Monad m => (a -> b) -> m a -> m b
liftM f m = m >>= \x -> return (f x)
\end{code}

\noindent
In the lambda, \hs{f} is a free variable so it becomes an argument to
the new top level function called \hs{lambda} below:

\begin{code}
lambda f x = return (f x)

liftM :: Monad m => (a -> b) -> m a -> m b
liftM f m = m >>= lambda f
\end{code}

A similar translation is done for sections.

In the rest of this thesis we shall write code with pattern matching
on arguments directly.

\section{Pattern Matching Revisited}
\label{sec:patternsrevisited}

\paragraph{Overlapping patterns} These needs to be removed to prevent
generation of inconsistent theories. Example:

\begin{code}
overlap :: Bool -> Bool
overlap b = case b of
              True -> True
              True -> False
\end{code}

Certainly, this cannot be translated to:
\begin{align*}
\rom{1} \qquad & \fn{overlap}(\fn{true}) = \fn{true} \\
\rom{2} \qquad & \fn{overlap}(\fn{true}) = \fn{false} \\
\rom{3} \qquad & \fa{b} b \neq \fn{true} \rightarrow \fn{overlap}(b) = \bot
\end{align*}

Reflexivity gives $\fn{overlap}(\fn{true}) = \fn{overlap}(\fn{true})$,
transitivity of the equalities in the axioms $\romnodot{1}$ and
$\romnodot{2}$ gives that $\fn{true} = \fn{false}$. Together with the
axiom from disjoint constructors, $\fn{true} \neq \fn{false}$, we have
a contradiction.

In Haskell, pattern matching is done from top to bottom of the
definition, making the second match of \hs{True} to never occur. Thus,
the translation removes all patterns that are instances of a pattern
above.



\paragraph{Nested patterns and bottoms} The translation also handles
patterns in more than one depth. At every location in a pattern where
a constructor is matched against, a pattern with bottom at that spot
is also added, defined to bottom. This Haskell function \hs{even}
determines if a list is of even length:

\begin{code}
even :: List a -> Bool
even (Cons x (Cons y ys)) = even ys
even (Cons x xs)          = False
even Nil                  = True
\end{code}

\noindent
For the sake of readability it is not presented with a case
expression, though this is what the intermediate translation would
transform it to. Furthermore the constructors \hs{Cons} and \hs{Nil}
are used since the selectors $\fn{:_0}$ and $\fn{:_1}$ for the
normal cons are hard to read.

Here, \hs{even} should return $\bot$ when it is evaluated with an
argument constructed with neither \hs{Cons} nor \hs{Nil} (recall that
the logic is untyped.) This undefined value should also be returned if
applied to $\hs{Cons x\w}\bot$ for some \hs{x}, since the \hs{Cons}
constructor is matched again on depth two. So there are two different
situations at each depth. One is if there is a match any pattern (for
\hs{even}, it is the variable \hs{xs} in the second pattern), new
patterns are added that matches $\bot$. The other is if there is no
wild pattern, a new one is added that goes to $\bot$.

\begin{comment}
First, it needs to be determined if there is a match-anything branch or not.
For \hs{even} above, there is no match anything case, so a new one is added
that matches anything that is not

For each matched constructor, we need to add a new match to bottom,
which evaluates to bottom. Unnecessary bottoms can be carelessly added
since overlapping patterns are removed \emph{afterwards}. Furthermore,
a wild pattern is added at the end that goes to bottom in case there
are other constructors for the data type not mentioned in the
patterns.
\end{comment}

No type information is needed to do this, only inspection of the
patterns. Could the bottoms be seen in this Haskell definition it
would be this after the insertion:

\begin{code}[mathescape]
even :: List a -> Bool
even (Cons x (Cons y ys)) = even ys
even (Cons x $\bot$)            = $\bot$
even (Cons x xs)          = False
even Nil                  = True
even _                    = $\bot$
\end{code}

Haskell's behavior of matching patterns from top to bottom is
justified with implications ensuring the \emph{upward agreement}. The
axioms for this definitions are:
\newcommand\uncons[1]{\cons{\fn{cons_0}(#1)}{\fn{cons_1}(#1)}}
\newcommand\even[1]{\fn{even}(#1)}
\newcommand\cons[2]{\fn{cons}(#1,#2)}
\begin{align*}
\rom{1} && \faaa{x}{y}{ys} & \even{\cons{x}{\cons{y}{ys}}} = \even{ys} \\
\rom{2} && \fa{x}          & \even{\cons{x}{\bot}}         = \bot      \\
\rom{3} && \faa{x}{xs}     & xs \neq \uncons{xs} \wedge xs \neq \bot \rightarrow \even{\cons{x}{xs}} = \fn{false}  \\
\rom{4} &&                 & \even{\fn{nil}} = \fn{true} \\
\rom{5} && \fa{xs}         & xs \neq \fn{nil} \wedge
                             xs \neq \uncons{xs}
                             \rightarrow \even{xs} = \bot
\end{align*}

\begin{comment}
Some room for improvement can be seen: the inserted
\hs{even }$\bot$\hs{ = }$\bot$ case is redundant as it is implied by
the wild pattern to $\bot$.
\end{comment}

The implications due to upward agreement are present in axioms
$\romnodot{3}$ and $\romnodot{5}$. This is needed for all wild
patterns.

\section{Functions as Arguments}

In Haskell, functions readily take other functions as arguments, and
functions can also be partially applied. To get the same behavior in
logic, each function is assigned a \emph{function pointer}, and a new
binary function is added to the language, written infix as $\appfn$.
For instance if there is a binary function \hs{plus} then a constant
called $\fn{plus.ptr}$ is added to the theory and this axiom:

\begin{equation*}
\faa{x}{y}  \app{(\app{\fn{plus.ptr}}{x})}{y} = \fn{plus}(x,y)
\end{equation*}

When a function is only partially applied, or a function argument is
applied, $\appfn$ is used. Consider this Prelude function \hs{iterate}:

\begin{code}
iterate :: (a -> a) -> a -> [a]
iterate f x = x : iterate f (f x)
\end{code}

It is translated with $\appfn$ in the following way, with the cons
constructor \hs{:} written infix:

\begin{equation*}
\forall \, f \, x \, . \, \fn{iterate}(f,x) = x : \fn{iterate}(f,\app{f}{x})
\end{equation*}

Should a function not get all its arguments, appropriate use of $\, @ \, $ is
added, as in this function which increments all elements of the list
by one using \hs{map}:

\begin{code}
incr = map (plus one)
\end{code}

As \hs{incr} is written $\eta$-reduced, \hs{map} is
only partially applied, this is the translated axiom:

\begin{equation*}
\fn{incr} = \app{\fn{map.ptr}}{(\app{\fn{plus.ptr}}{\fn{one}})}
\end{equation*}

If \hs{incr} is applied to an argument $xs$, then \hs{incr} is applied
to more arguments than it takes, so we add $\appfn$ so the
corresponding formula becomes $\app{\fn{incr}}{xs}$, and by equational
substitution from the definition of $\fn{incr}$ we get
$\app{(\app{\fn{map.ptr}}{(\app{\fn{plus.ptr}}{\fn{one}})})}{xs}$ and
the axiom of $\fn{map.ptr}$ then equals this to
$\fn{map}(\app{\fn{plus.ptr}}{\fn{one}},xs)$.

\paragraph{Doing the impossible}
While it is not possible to quantify over functions in first order
logic, this translation allows universal quantification of functions,
allowing a way to reason syntactically about partially applied
functions. On the model side, $\appfn$ gives a way to interpret
functions and universally quantify over them. If the function has a
pointer defined, it just constrains $\appfn$ on that pointer to do the
same as the function.

\section{Guards}

Guards are treated similar to pattern matching. If a guard expression
evaluates to \hs{True}, that branch is picked. The expression could
also evaluate to $\bot$, and then the result should be $\bot$. Let us
consider the \hs{filter} function:

\begin{code}
filter :: (a -> Bool) -> List a -> List a
filter p (Cons x xs) | p x = Cons x (filter p xs)
filter p (Cons x xs)       = filter p xs
filter p Nil               = Nil
\end{code}


To translation this to logic it is needed to ensure that if \hs{p x}
evaluates to $\bot$, then so should the function. The axioms look
like this:
\newcommand\filter[2]{\fn{filter}(#1,#2)}
\begin{align*}
\rom{1} && \faaa{p}{x}{xs} & (\app{p}{x}) = \fn{true}                                  \rightarrow \filter{p}{\cons{x}{\cons{y}{ys}}} = \cons{x}{\filter{p}{xs}} \\
\rom{2} && \faaa{p}{x}{xs} & (\app{p}{x}) = \bot                                       \rightarrow \filter{p}{\cons{x}{\cons{y}{ys}}} = \bot \\
\rom{3} && \faaa{p}{x}{xs} & (\app{p}{x}) \neq \fn{true} \wedge (\app{p}{x}) \neq \bot \rightarrow \filter{p}{\cons{x}{\cons{y}{ys}}} = \filter{p}{xs} \\
\rom{4} &&                 & \filter{p}{\fn{nil}} = \fn{nil} \\
\rom{5} && \fa{xs}         & xs \neq \fn{nil} \wedge xs \neq \uncons{xs} \rightarrow \filter{p}{xs} = \bot
\end{align*}

\section{Summary}

The translation of different Haskell concepts is summarised in
Table~\ref{tab:transtable}.

\begin{table}[h]
  \centering
  \begin{tabular}{|l|l|}
    \hline
    Haskell                    & First Order Logic \\
    \hline
    function                   & function or constant \\
    constructor                & function or constant \\
    data type                  & disjoint constructors and selector axioms \\
    pattern matching           & overlap removal, bottoms insertion, upward agreement \\
    guards                     & equality to true and bottom and upward agreement \\
    partial application        & $\appfn$ on pointer constant \\
    partially applied function & pointer constant and $\appfn$ rule \\
    sections, lambdas, let     & new functions with variables in scope as arguments \\
    \hline
  \end{tabular}
  \caption{Translation of different Haskell constructs
    \label{tab:transtable}
  }
\end{table}

%Equational reasaoning is traditional in proving corrected of Haskell
%programs, but it assumes that a simple denotational semantics exists,
%and there is not even a formal semantics for the language
%\cite{chasingbot}.
%

% Remove unnecessary definitions for a given proof


%% Domain theory

\input{domaintheory}

\section{Future Work}

Haskell is a big language, and translating it all in one go is a
daunting task. Therefore, some restrictions were settled to be able to
focus on proving rather than translating.  The goal was to add enough
of the Haskell language to enable to prove interesting properties, but
much of the widely available sugar in Haskell was omitted since it
does not add extra expressibility. This means that list comprehensions
and are not supported but can be added by their respective rewriting
rules. \hs{Type} definitions should be unrolled, so they could be
used in the signature for properties. Type classes is probably the
most interesting thing to add, and is discussed in Section
\ref{sec:typeclasses} in future work.

Another interesting but omitted feature are the built-in types
\hs{Int}, \hs{Integer}, \hs{Double}, \hs{Char}, etc. For \hs{Integer}
appropriate axioms could be added that hold for $\mathbb{Z}$, the
canonical infinite discretely ordered commutative ring.  The other
data types do not enjoy such well behaved properties because of
different bit sizes and overflow and precision errors.

Syntactic features for controlling lazy and strict evaluation namely
irrefutable patterns, \hs{seq} and bang patterns, and richer pattern
matching in form of pattern bindings are discussed below.

\begin{comment}
 It should be
noted that it is already possible to prove a lot of interesting
Haskell properties, it is far from able to prove things about bigger
Haskell projects which usually use a richer part of the language.
\end{comment}

\subsection{Irrefutable Patterns and Pattern Bindings}

Irrefutable patterns can be defined in terms of projections, examples
are \hs{fst}, \hs{snd}, \hs{head}, \hs{fromJust} defined in the
standard library. Each irrefutable pattern is translated to a
constant, and when you use the variables in the pattern, you translate
it to appropriate use of projections. One example is the translation
of the \hs{uncurry} function:

\begin{code}[mathescape]
uncurry f ~(x,y) = f x y        $\Rightarrow$      uncurry f t = f (fst t) (snd t)
\end{code}

\noindent
The irrefutable pattern \verb:~(x,y): is replaced with the new constant
\hs{t}, and in the body of the function, \hs{x} is replaced with the
strict projection \hs{fst t}, and similarly for \hs{y}.

Top level patterns, also called pattern bindings, can
also be written in terms of such projections. The whole pattern
is replaced with a constant, and when the variables from the pattern
are used, you again replace it with projections. This is how it
could look for a simple \hs{fromJust . lookup} implementation:

\begin{code}[mathescape]
unsafeLookup x xs = v           $\Rightarrow$      unsafeLookup x xs = fromJust t
  where Just v = lookup x xs            where t = lookup x xs
\end{code}

The strict projections would not rely on the user having \hs{fst} or
\hs{fromJust} in scope, they can automatically be generated by
inspection of the data type definition.

\subsection{Bang Patterns and seq}

The translations for bang patterns and \hs{seq} are also
straightforward. \hs{seq} defined by bang patterns is:

\begin{code}
seq :: a -> b -> b
seq !x y = y
\end{code}

The axioms for a translation of \hs{seq} needs to ensure that if
\hs{x} evaluates to $\bot$, then \hs{seq x} also evaluates to
$\bot$. The two axioms for this functions are:
\begin{align*}
\rom{1} \qquad & \fa{y}    seq(\bot,y) \eq \bot \\
\rom{2} \qquad & \faa{x}{y} x \neq \bot \rightarrow seq(x,y) \eq y
\end{align*}

Either you implement bang patterns in this fashion, or you do the same
translation as GHC for bang patterns: for each strict variable, you
add a \hs{seq} for that variable for the expression of that pattern,
and you simply add the axioms for \hs{seq} to the theory if the
program uses it or bang patterns. For data types with strictness
fields one proceeds by adding \hs{seq} when constructing elements.

\subsection{Pattern Guards}

Patterns guards is a GHC specific extension to Haskell which allows
arbitrary pattern matching on the result from an expression in a
guard. An example is this elaboration of the \hs{lookup} function from
the \hs{Prelude}, which applies a function to the element, if found:

\begin{code}
transformLookup :: Eq k => k -> [(k,v)] -> (k -> v -> b) -> Maybe b
transformLookup k xs f | Just v <- lookup k xs = Just (f k v)
                       | otherwise             = Nothing
\end{code}

\noindent
If the lookup returns \hs{Just}, \hs{v} is already bound and can be
used in the expression of the right hand side. This is very simiar to
normal guards, as they are a special case of pattern guards: the guard
\hs{f x | p x} is expressed as \hs{f x | True <- p x}. The current
translation of guards checks if \hs{p x} is \hs{True}, and then
``picks'' this branch, or is $\bot$. This could be done for
constructors, bottoms would need to be added in the guard branches as
is currently done for ordinary patterns.

