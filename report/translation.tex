\chapter{Haskell to First Order Logic}

To enable automated theorem provers to do equational reasoning of
Haskell programs, we translate them to first order logic. It is
referred to as a translation, but it could also be regarded as a
compilation. The idea is to use constants and functions in first order
logic to correspond to constructors and functions, and arguments to
functions need to be universally quantified. We shall try to do a
na\"{\i}ve attempt of a translation with this ideas and see how far it
takes us.

\section{Na\"{\i}ve translation}

We will use a data type of binary trees with an element at every
branch, and consider some examples of functions defined on it. This
is the Haskell definition of the data type we will be using:

\begin{lstlisting}
data Tree a = Fork (Tree a) a (Tree a) | Leaf
\end{lstlisting}
With the idea above, occurrences of the \hs{Fork} constructor in the
source code should be translated to a logic function $\fn{fork}$, and
similarly a constant for \hs{Leaf}. How should we then translate the
\hs{singleton} function, defined below?

\begin{lstlisting}
singleton :: a -> Tree a
singleton x = Fork Leaf x Leaf
\end{lstlisting}

\noindent
Following our intuition and make an universal quantification for
\hs{x}, and make a new logic function for \hs{singleton}. The result
is this axiom:
\begin{equation*}
\fa{x} \fn{singleton}(x) = \fn{fork}(\fn{leaf},x,\fn{leaf})
\end{equation*}

\noindent
So far so good, but what if someone wants to prove that \hs{singleton x}
is a \hs{Leaf}? With only this axiom in the theory, it would be
possible: there are models with only one element where \hs{Leaf} is
equal to \hs{Fork}. Indeed, we will need to add axioms that values
created from the different constructors are unequal. We will call
those disjoint constructor axioms, and for the \hs{Tree} data type, we
get this axiom:
\begin{equation*}
\faaa{l}{x}{r} \fn{leaf} \neq \fn{fork}(l,x,r)
\end{equation*}

Constructors should also be injective to get regular models, and
adding such axioms are straightforward. Since \hs{Fork} has arguments,
it needs an injectivity axiom:
\begin{equation*}
\faaaaaa{l_0}{l_1}{x_0}{x_1}{r_0}{r_1} \fn{fork}(l_0,x_0,r_0) \eq
\fn{fork}(l_1,x_1,r_1) \rightarrow l_0 \eq l_1 \wedge x_0 \eq x_1 \wedge r_0 \eq r_1
\end{equation*}

For the \hs{mirror} function, which recursively mirrors the left sub
tree with the right and vice-versa, we follow our intuition to
translate the pattern matching and get these two axioms\footnote
{Axioms are enumerated by Roman numerals to tell them apart.}:

\begin{lstlisting}
mirror :: Tree a -> Tree a
mirror (Fork l x r) = Fork (mirror r) x (mirror l)
mirror Leaf         = Leaf
\end{lstlisting}
\begin{align*}
\rom{1} && \faaa{l}{x}{r} & \fn{mirror}(\fn{fork}(l,x,r)) \eq \fn{fork}(\fn{mirror}(r),x,\fn{mirror}(l)) \\
\rom{2} &&                & \fn{mirror}(\fn{leaf}) \eq \fn{leaf}
\end{align*}

A problem with this translation is that there are no axioms for other
arguments of \fn{mirror} than leafs and forks, and we have models that
include other values than leafs and forks. Another problem is
encountered for \hs{singleton}'s left inverse, \hs{top}, code below,
which returns the top element of a \hs{Tree}. This is a partial
function since it does not cover all patterns.

\begin{lstlisting}[label=lst:top]
top :: Tree a -> a
top (Fork l x r) = x
\end{lstlisting}

The translation must capture the pattern match failure that results
from trying to evaluate top applied to a leaf. We conclude that
this na\"{\i}ve translation does not take us further, but we shall see
in the next section how to fix this.

\section{Bottom and pattern matching}

In domain theory there is a concept of bottom, denoted $\bot$. It is
used for the least defined value: pattern match failures, use of
\hs{error} and \hs{undefined} in the source code, but also for
non-terminating programs.  \note{Refer to a short introduction of
  domain theory at the end of this chapter (or elsewhere, appendix?)}
For \hs{top} the idea is to add an axiom so that \fn{top} of anything
that is not a \hs{Fork} is bottom. This is an example of such an
axiomatization:
\begin{align*}
\rom{1} && \faaa{l}{x}{r} & \fn{top}(\fn{fork}(l,x,r)) \eq \fn{fork}(\fn{mirror}(r),x,\fn{mirror}(l)) \\
\rom{2} && \fa{i}         & (\nexxx{l}{x}{r} \fn{fork}(l,x,r)) \eq i) \rightarrow \fn{top}(i) \eq \bot
\end{align*}

Most theorem provers would \note{citation needed}skolemize the second
axiom. A new unary function would be introduced for $l$, $x$ and $r$,
depending on $i$, an arbitrary choice of names are $\fn{top}$
appended to the original variable. The axiom becomes:
\begin{align*}
\rom{2}' && \fa{i} & (\fn{fork}(\fn{topl}(i),\fn{topx}(i),\fn{topr}(i))) \neq i) \rightarrow \fn{top}(i) \eq \bot
\end{align*}

For another function, like \hs{mirror} above, the skolemized function
could be called $\fn{mirror}$ and the variable name. Since axioms of
injective constructors are also added, we could conclude that
$\faaa{l}{x}{r} \fn{mirrorl}(\fn{fork}(l,x,r)) \eq
\fn{topl}(\fn{fork}(l,x,r)) \eq l$. But what happens if we introduce
such skolemized ``selector'' functions for every constructor manually?
We call them $\fn{fork_0}$ up to $\fn{fork_2}$, and their axioms are:
\begin{align*}
\rom{1} \qquad \faaa{l}{x}{r} \fn{fork_{0}}(\fn{fork}(l,x,r)) & \eq l\\
\rom{2} \qquad \faaa{l}{x}{r} \fn{fork_{1}}(\fn{fork}(l,x,r)) & \eq x\\
\rom{3} \qquad \faaa{l}{x}{r} \fn{fork_{2}}(\fn{fork}(l,x,r)) & \eq r\\
\end{align*}
The translation of \hs{top} then becomes:
\begin{align*}
\rom{1} && \faaa{l}{x}{r} & \fn{top}(\fn{fork}(l,x,r)) \eq \fn{fork}(\fn{mirror}(r),x,\fn{mirror}(l)) \\
\rom{2} && \fa{i}         & (\fn{fork}(\fn{fork_0}(i),\fn{fork_1}(i),\fn{fork_2}(i))) \neq i) \rightarrow \fn{top}(i) \eq \bot
\end{align*}
Another nice side effect of writing in this skolemized selector style
is that implies injective constructors, for assume we have
$\fn{fork}(l_0,x_0,r_0) = \fn{fork}(l_1,x_1,r_1)$ then the first
projection, $\fn{fork_0}$, gives us that $l_0=l_1$. Analogously,
and the second and the third give $x_0=x_1$ and $r_0=r_1$,
respectively. So we add selector axioms instead of injectivity
axioms.

Now we have a good idea how to translate pattern matching, but
in Haskell we can pattern match almost everywhere! How would we
proceed to translate a function like this, taken from the
implementation of \hs{scanr} in the \hs{Prelude}?

\begin{lstlisting}
scanr             :: (a -> b -> b) -> b -> [a] -> [b]
scanr f q0 []     =  [q0]
scanr f q0 (x:xs) =  f x q : qs
                     where qs = scanr f q0 xs
                           q = case qs of
                                 q : _ -> q
\end{lstlisting}

There is both pattern matching on the direct arguments, but also
pattern matching in a case statements in the where function
\hs{q}. There can also be pattern matching in lambdas. To help with
these difficulties, we define an intermediate language in the next
section.

\section{The Intermediate Language}

To address the difficulties of pattern matching elsewhere than the
arguments of a function, a little small intermediate language is
designed that can only do pattern matching at a very controlled
location: in a case statement that is the entire body of a function,
and all arms are just simple expressions consisting of function and
constructor applications and variables. Haskell is translated to this,
and pattern matching at other locations is translated to this in a new
top level definition. Functions definined in \hs{let} and \hs{where}
are raised to the top level, with the necessary variables in scope as
additional arguments. The same is done for sections and lambda
functions. The BNF for the language is this:

\begin{equation*}
\begin{aligned}
\text{Variables} \quad & x,f \\
\text{Constructors} \quad & C \\
\text{Type variables} \quad & \tau \\
\text{Type constructors} \quad & T \\
\defBNF{Declarations}{decl}{ f \; \overline{x} \; \hs{=} \; body}{function declaration} \\
    \defaltBNF{f \; \hs{::} \; t}{type signature} \\
    \defaltBNF{\hs{data} \; T \; \overline{\tau} \; \hs{=} \; \overline{C \; \overline{t}}}{data type declaration} \\
\defBNF{Function body}{body}{\hs{case} \; e \; \hs{of} \; \overline{alt}}{case body} \\
    \defaltBNF{e}{expression body} \\
\defBNF{Expressions}{e}{x}{variable} \\
    \defaltBNF{f \; \overline{e}}{function application} \\
    \defaltBNF{C \; \overline{e}}{constructor application} \\
\defBNF{Alternative}{alt}{pat \rightarrow e}{branch without guard} \\
    \defaltBNF{pat \; \hs{|} \; e \rightarrow e}{branch with guard} \\
\defBNF{Pattern}{p}{x}{pattern variable} \\
    \defaltBNF{C \; \overline{p}}{constructor pattern} \\
\defBNF{Types}{t}{\tau}{type variable} \\
    \defaltBNF{t \; \rightarrow \; t}{function type} \\
    \defaltBNF{T \; \overline{\tau}}{type constructor application} \\
\defBNF{Programs}{prog}{\overline{decl}}{} \\
\end{aligned}
\end{equation*}




\section{Summary}

We summarize the translation in Table~\ref{tab:transtable}.

\begin{table}[h]
  \centering
  \begin{tabular}{|l|l|}
    \hline
    Haskell                    & First Order Logic \\
    \hline
    function                   & function or constant \\
    constructor                & function or constant \\
    data type                  & disjoint constructors and selector axioms \\
    pattern matching           & insertion of bottoms and upwards axioms \\
    guards                     & equality to true and bottom plus upwards axioms \\
    partial application        & $\appfn$ on pointer constant \\
    partially applied function & pointer constant and $\appfn$ rule \\
    sections, lambdas, let     & new functions with variables in scope as arguments \\
    \hline
  \end{tabular}
  \caption{Translation of different Haskell constructs
    \label{tab:transtable}
  }
\end{table}

The goal of this translation is to enable equational reasoning of
Haskell programs, in FOL. It is here referred to as a translation, but
it could also in be regarded as compilation.

Equational reasoning is traditional in proving corrected of Haskell
programs, but it assumes that a simple denotational semantics exists,
and there is not even a formal semantics for the language
\cite{chasingbot}.

\section{Haskell}

Describe some of the features of Haskell, types, lazy evaluation,
bottoms, pattern-matching, data types, type classes.

Haskell is declarative and side-effect free which makes it easy
to model in logic in comparison to imperative effectful programming languages.

\section{Domains}

This is the right place to introduce some notions of domain theory:
values of a data type form a complete partial order with least element
$\bot$, and introduce concepts of monotonicity,
$\omega$-$\sqsubseteq$-limits and continuity.

% Whoa, much in this section
\section{Data types}
\label{sec:treetrans}

Haskell's data types and its consequence pattern matching is
translated by giving each constructor a function or constant symbol.

Consider this examples of binary trees in Haskell style:

\begin{verbatim}
data Tree a = Empty | Branch (Tree a) a (Tree a)
\end{verbatim}

We get one constant $\fn{Empty}$ for \hs{Empty}, and one trinary
function $\fn{Branch}$ for \hs{Branch}. Normally, it is custom
to write functions in logic with lowercase letters, but this
convention is disregarded here. The values of these constructors are
distinct, so the following disjointedness axioms are added to the
theory of a Haskell program with this data type.

\begin{align*}
\fn{\bot} &  \neq \fn{Empty}\\
 \forall \, l \,  x \,  r \,  . \,  \fn{\bot} &  \neq \fn{Branch}(l,x,r)\\
 \forall \, l \,  x \,  r \,  . \,  \fn{Empty} &  \neq \fn{Branch}(l,x,r)
\end{align*}

Notice that in Haskell, each data type also has an extra value, bottom, which
can come from the functions \hs{undefined} or \hs{error}, as well as
irrefutable pattern matches but also non-terminating programs. The constant
$\bot$ captures this notion and is naturally also distinct from the values
of \hs{Empty} and \hs{Branch}.

\section{Injective constructors and projections}

We also want injectivity of constructors, for example if we have the
cons constructor, \hs{:} in Haskell, and \hs{x:xs = y:ys} then
\hs{x = y} and \hs{xs = ys}. As we will see later, we also want the
projections of the left and right sub tree, and the value in a Branch.
It turns out that the projections imply the injectivity. For the Branch
constructor of the Tree example, we get the following projections:

\begin{align*}
 \forall \, l \, x \, r \, . \,  \fn{Branch_{0}}(\fn{Branch}(l,x,r)) &  = l\\
 \forall \, l \, x \, r \, . \,  \fn{Branch_{1}}(\fn{Branch}(l,x,r)) &  = x\\
 \forall \, l \, x \, r \, . \,  \fn{Branch_{2}}(\fn{Branch}(l,x,r)) &  = r\\
\end{align*}

These ternary functions are bluntly named by indexing on the projected
coordinate, but could just as well have more descriptive names as
$\fn{left}$, $\fn{value}$ and $\fn{right}$.

Now, these projections imply injective constructors. Assume we have
$\fn{Branch}(l,x,r) = \fn{Branch}(l',x',r')$ then the first
projection, $\fn{Branch_0}$, gives us that $l=l'$. Analogously,
and the second and the third give $x=x'$ and $r=r'$, respectively.

\section{Translation of functions}

Translating functions that do not use pattern matching is
straightforward. Consider this Haskell definition of a singleton tree:

\begin{verbatim}
leaf :: a -> Tree a
leaf x = Branch Empty x Empty
\end{verbatim}

We simply introduce a new function symbol, $\fn{leaf}$, and due
to referential transparency we can turn the definition into an
equality with quantified variables:

\begin{align*}
 \forall \, x \, . \, \fn{leaf}(x) &  = \fn{Branch}(\fn{Empty},x,\fn{Empty})
\end{align*}

\section{Pattern matching}

Let's now consider \hs{leaf}'s partial inverse, \hs{top} that
yields the top element of the tree if there is one, or is undefined
otherwise:

\begin{verbatim}
top :: Tree a -> a
top (Branch _ x _) = x
\end{verbatim}

The function call of \hs{top Empty} would yield a run time error
since this pattern is not covered, equivalent to an undefined
value. Indeed, an equivalent formulation would be to cover the
\hs{Empty} case by \hs{undefined} or a helpful message from
\hs{error}, but the run time error is still remains, and this
behavior is modeled by the $\bot$ value in the theory. Thus the
translation to an equality is as follows: if the argument is
constructed with \hs{Branch}, it is equivalent to the top value,
otherwise, it is $\bot$:

\begin{align*}
 \forall \, l \, x \, r \, & . \, \fn{top}(\fn{Branch}(l,x,r) = x\\
 \forall \, t \, & . \, t \neq
 \fn{Branch}(\fn{Branch_{0}}(t),\fn{Branch_{1}}(t),\fn{Branch_{2}}(t))
 \rightarrow \, \fn{top}(t)  = \fn{\bot}
\end{align*}

Here the projections functions come in handy. Indeed, an equivalent
but skolemized formulations of the second formulas are:

\begin{align*}
 \forall \, t \, & . \neg (\exists \, l \, x \, r . \, t =
 \fn{Branch}(l,x,r))
 \rightarrow \, \fn{top}(t) = \bot \\
 \forall \, t \, & . (\forall \, l \, x \, r . \, t \neq
 \fn{Branch}(l,x,r))
 \rightarrow \, \fn{top}(t) = \bot
\end{align*}

But as we saw earlier, projections also imply injectivity so this is
the approach used here but the choice is of little importance (or is
it? benchmark!!)

\section{Overlapping patterns}

Overlapping patterns need to be removed, otherwise we could easily get
an inconsistent theory, consider

\begin{verbatim}
overlap :: Bool -> Bool
overlap True = True
overlap True = False
\end{verbatim}

Certainly, we cannot translate this to
\begin{align*}
\fn{overlap}(\fn{True}) & = \fn{True} \\
\fn{overlap}(\fn{True}) & = \fn{False} \\
\forall \, b \, . \, b \neq True & \rightarrow \fn{overlap}(b) = \bot
\end{align*}

Transitivity of equality then yields $\fn{True} = \fn{False}$,
and this together with the axioms of disjoint constructors gives a
contradiction.

In Haskell, pattern matching is done from top to bottom of the
definition, making the second match of True to never occur. Thus, the
translation to FOL also removes all subsequent patterns that are
instances of any pattern above.

\section{Nested patterns and bottoms}

The translation also handles patterns in more than one depth. At every
location in a pattern where a constructor is matched against, a
pattern with bottom at that spot is also added, defined to
bottom. This definition that balances a tree to the left is defined
with pattern matching on depth two:

\begin{verbatim}
unbalance :: Tree a -> Tree a
unbalance (Branch (Branch l x r) y r') = unbalance (Branch l x (Branch r y r'))
unbalance (Branch l x r)               = Branch l x (unbalance r)
unbalance Empty                        = Empty
\end{verbatim}

If we could see the bottoms in Haskell, the definition would look like this:

\begin{verbatim}
unbalance :: Tree a -> Tree a
unbalance (Branch (Branch l x r) y r') = unbalance (Branch l x (Branch r y r'))
unbalance (Branch Bottom _ _)          = Bottom
unbalance (Branch l x r)               = Branch l x (unbalance r)
unbalance Empty                        = Empty
unbalance Bottom                       = Bottom
\end{verbatim}

And such an addition of bottoms is made by the translation.
\note{This writing actually discovered a bug. Report the solution and how it was tested}

\section{Guards}

Guards are not much of a complication. Either the guard expression is
\hs{True}, then that branch is picked. If the expression returns
bottom, then for this argument, the function is bottom. Care needs to
be taken when looking ``upwards'' the branches, to not collide with
the guards.
\note{Add example}

\section{Functions as arguments}

In Haskell, functions readily take other functions as arguments, and
functions can also be partially applied. To get the same behavior in
logic, each function gets a \emph{function pointer}, and a new binary
function is added to the language, written infix with $\fn{@}$.
\note{Should $\fn{@}$ be written infix? $\fn{app}$ prefix is also a viable option}
For instance, the if there is a binary function plus then a constant
called plus-ptr is added to the theory and this axiom:

\begin{equation*}
\forall \, x \, y \, . \, \app{(\app{\fn{plus.ptr}}{x})}{y} = \fn{plus}(x,y)
\end{equation*}

When a function is only partially applied, or a function argument is
applied, $\, @ \, $ is used. Consider the Prelude function \hs{iterate}

\begin{verbatim}
iterate :: (a -> a) -> a -> [a]
iterate f x = x : iterate f (f x)
\end{verbatim}

It is translated with $\, @ \, $ in the following way, with \hs{:} written infix:

\begin{equation*}
\forall \, f \, x \, . \, \fn{iterate}(f,x) = x : \fn{iterate}(f,\app{f}{x})
\end{equation*}

Should a function not get all its arguments, appropriate use of $\, @ \, $ is
added, as in this function which increments all elements of the list
by one using \hs{map}:

\begin{verbatim}
incr = map (plus one)
\end{verbatim}

As \hs{incr} is also written point-free or eta-reduced, \hs{map} is
only partially applied. This is the translated axiom:

\begin{equation*}
\fn{incr} = \app{\fn{map.ptr}}{(\app{\fn{plus.ptr}}{\fn{one}})}
\end{equation*}

If \hs{incr} is applied to an argument $xs$, then \hs{incr} is applied
to more arguments then it takes, so we add $\, @ \,$ so the
corresponding formula becomes $\app{\fn{incr}}{xs}$, and by equational
substitution from the definition of $\fn{incr}$ we get
$\app{(\app{\fn{map.ptr}}{(\app{\fn{plus.ptr}}{\fn{one}})})}{xs}$ and
the axiom of $\fn{map.ptr}$ then equals this to
$\fn{map}(\app{\fn{plus.ptr}}{\fn{one}},xs)$.

\section{The intermediate language}

Everything becomes top level definitions, \hs{let} and \hs{where} are
floated to the top, with the free variables added as extra parameters,
as well as lambda functions, the same approach was taken for extra or
nested case-expressions. Extra care needs to be taken for mutually
recursive \hs{let} and \hs{where} functions, which potentially need's
each other's free variables as arguments.
\note{Incomplete, add examples}

Functions do not do any pattern matching on their arguments directly,
but in a case statement that is the entire body of the function. The
branch expressions are just constructor or function application.
\note{Add BNF}

Type-signatures and data types are also supported since this
information is needed for different proof techniques.

\section{Haskell coverage}

Haskell is a big language, and translating it all in one go is a
daunting task. Therefore, some restrictions were settled to be able to
focus on proving rather than translating.
\note{Maybe move this to background?}
The goal was to add enough of the Haskell language to enable to prove
interesting properties, but much of the widely available sugar in
Haskell was omitted since it does not add extra
expressibility. Therefore there is no support for
list comprehensions, do-notation, pattern bindings and type classes .

A more serious omitted feature is the lack of built-in types like \hs{Int},
\hs{Integer}, \hs{Double}, \hs{Char}, etc.

(Higher-kinded type variables are currently not supported)

\section{Uncategorized}

\begin{itemize}

\item $\checkmark$ Describe (and motivate here?) the intermediate language

\item $\checkmark$ Pattern-matching and bottoms

\item $\checkmark$ Higher order functions and function pointers

\item $\checkmark$ Axioms of disjointedness

\item $\checkmark$ Axioms of projections and injectivity of constructors

\item $\checkmark$ Extensional equality and application of bottom

\item Remove unnecessary definitions for a given proof

\end{itemize}
