\section{Future work}

The current work has some irritating limitations that we would like to address.

\subsection{Lemmas}

For many proofs, it is essential to use some lemma to reach the
goal. For commutativity of plus, we already touched on this aspect, it
needs the left identity as well as the $s(n) + m \eq n + s(m)$
lemma. Furthermore, to prove associativity of multiplication you need
at least associativity of addition.

Implementing addition of lemmas for properties that always hold, i.e,
for also partial and infinite values is straightforward; you can just
add the universally quantified property to the theory. The problem is
when it only holds for finite or total values: you will need FOL
predicates or functions to describe exactly when the property
holds. Research has been made how to use such predicates
\cite{sortMonotonicity}, \cite{polyMonotonicity}, but it is not
decided yet on which way to encode types. Furthermore, you will need
to prove that functions return finite or total values on such
input. For instance, append for lists gives a total list if both input
lists are total, but on the other hand, if the result list is total
but potentially infinite, we can only draw the conclusion that the
first list is total, since that might be the infinite one. So, to
prove properties that does not always hold you will need to prove a
lot more properties about your functions.

Another way to reason about this is to assume that the undefined value
does not exist, and this could be deemed as morally correct
\cite{fastandloose}. Then you cannot use fixed point induction but you
can still use structural induction.

\note{Discuss this with Moa and get references}
Another aspect of this is where to get the lemmas from. In the Isabel
induction and rippling community, there are concepts such as critics,
which makes lemmas and generalizations when the term rewriting
heuristic rippling fails, and lemma calculation, which tries to
syntactically analyze the problem before proving in order to figure
out suitable lemmas. Another approach is to do property speculation,
implemented for Isabell \cite{isacosy} and Haskell \cite{quickspec},
where you simply tell the QuickSpec program all functions in your
program, and it tries to find equality properties by creating small syntax
trees of the functions. It would be very interesting to see how useful
such properties are as lemmas.

The third aspect is how should the user inform the program which
lemmas are needed? One solution is to allow the user to annotate the
source code where the properties are entered which lemmas might be
appropriate to use. Another is to first try to prove all properties,
and for those which did not turn out to be a theorem, add all the
succeeded properties as a lemma and iterate until you reach a fixed
point. This could probably be optimized: if a lemma only concerns
functions that are not relevant for a given property, it is
unnecessary to add it to the theory.

\subsection{Material implication}

Implications

\subsection{Proving non termination}

Since we will need to prove termination for functions, how would one
prove non-termination (for some inputs)? Then we could get more
completeness: this function with these inputs would be $\bot$.

\subsection{Extending techniques}

Each of the proof methods have their own future work chapter, but it
would be nice to have some finite fixed point induction on terminating
functions and a finite approximation lemma.

You can also do as with the approximation lemma which became just a
fixpoint induction over a structural identity function, but instead of
putting one of each side of the equality, you can do it on the
individual variables or sub trees of an expression: this would be like
structural induction encoded as approximation lemma/fixed point induction.

Another technique is recursion induction \cite{recind}, where you
prove that two functions are equal by asserting that one of them
fulfills the same equations as the other.
\note{elaborate}

\subsection{More of Haskell}

There are still parts of Haskell that are not supported. List
comprehensions and do-notation can be added by its rewriting rules.
\hs{Type} definitions should be unrolled , so they could be used in
the signature for properties. Type classes is probably the most
interesting thing to add, and an approach would be to use dictionary
passing, and inline for concrete types. However, more type information
would be needed but it is possible that much of it could be extracted
from example GHC.

Syntactic features for controlling lazy and strict evaluation like
irrefutable patterns, \hs{seq} and bang patterns, and richer pattern
matching in form of pattern bindings are discussed below, but it
should be noted that it is already possible to prove a lot of
interesting Haskell properties, it is far from able to prove things
about bigger Haskell projects which usually use a richer part of the
language.

\subsubsection{Irrefutable patterns and pattern bindings}

Irrefutable patterns can be defined in terms of strict projections,
not unlike those that already exist, \hs{fst}, \hs{snd}, \hs{head},
\hs{fromJust}, and so on. Each irrefutable pattern is translated to
a constant, and when you use the variables in the pattern, you
translate it to appropriate use of strict projections. One example is
the translation of the uncurry function:

\begin{verbatim}
uncurry f ~(x,y) = f x y    <=>  uncurry f t = f (fst t) (snd t)
\end{verbatim}

The irrefutable pattern \hs{~(x,y)} is replaced with the new constant
\hs{t}, and in the body of the function, \hs{x} is replaced with the
strict projection \hs{fst t}, and similarly for \hs{y}.

Top level patterns, more specifically called pattern bindings, can
also be written in terms of such strict projections. The whole pattern
is replaced with a constant, and when the variables from the pattern
are used, you again replace it with strict projections. This is how it
could look for a simple \hs{fromJust . lookup} implementation:

\begin{verbatim}
unsafeLookup x xs = v           <=>   unsafeLookup x xs = fromJust t
  where Just v = lookup x xs            where t = lookup x xs
\end{verbatim}

The strict projections would not rely on the user having \hs{fst} or
\hs{fromJust} in scope, they can automatically be generated by
inspection of the data type definition.

\subsubsection{Bang patterns and seq}

The encoding for bang patterns and \hs{seq} is also straightforward,
say you want to define seq with bang patterns, you would have

\begin{verbatim}
seq :: a -> b -> b
seq !x y = y
\end{verbatim}

The axioms needs to ensure that if \hs{x} evaluates to $\bot$, then
\hs{seq x} also evaluates to $\bot$. The two axioms for this functions are:

\begin{align*}
\fa{y}    & seq(\bot,y) \eq \bot
& \faa{x}{y} x \neq \bot \rightarrow seq(x,y) \eq y
\end{align*}

Either you implement bang patterns in this fashion, or you do the same
translation as GHC for pang patterns: for each strict variable, you
add a \hs{seq} for that variable for the expression of that pattern,
and you simply add the axioms for \hs{seq} to the theory if the
program uses it or bang patterns. This also works for data types with
strictness fields.

\subsubsection{Pattern guards}

Patterns guards are a GHC specific extension to Haskell, that allows
you to pattern match against values returned from a function in a
guard. An example is this elaboration of the \hs{lookup} function from
the prelude, which applies a function to the found element:

\begin{verbatim}
transformLookup :: k -> [(k,v)] -> (k -> v -> b) -> Maybe b
transformLookup k xs f | Just v <- lookup k xs = Just (f k v)
                       | otherwise             = Nothing
\end{verbatim}

If the lookup returns \hs{Just}, you already have the value \hs{v}
bound and can use it in the expression for this pattern. This in not
at all unlike the normal guards, they are a special case of pattern
guards: the guard \hs{f x | p x} is expressed as
\hs{f x | True <- p x}. The translation of guards currently checks if
\hs{p x} is \hs{True}, and then ``picks'' this branch, or $\bot$ and
then ``returns'' $\bot$. You could just as well do this for other
constructors, you just need to add bottoms in the guard branches just
as is currently done for ordinary patterns.

\subsection{Faster proof searches via predicates}

\note{Ask Koen how this really works!}
It is possible to add annotations in the equations generated for
functions to make the theorem prover not unroll unnecessary
definitions and regard these equalities more like definitions, so they
do not rewrite from the wrong direction. One way is to add a
min-predicate (named so for historical reasons), that can also
sometimes lead to finite models.

