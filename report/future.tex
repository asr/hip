\chapter{Future work}
\label{ch:future}

The chapter addresses some limitations of our approach and ideas how
to get around them.

\section{Pattern Matching Re-Revisited}
\label{sec:rerevisited}

The current translation of pattern matching is described in Section
\ref{sec:patternsrevisited}. One of the strengths of it is the
translation of wild patterns. Consider this function for equality of a
data type with three elements:

\begin{code}
data Tri = A | B | C

equal :: Tri -> Tri -> Bool
equal A A = True
equal B B = True
equal C C = True
equal _ _ = False
\end{code}

\noindent
The translation adds two more cases that go to bottom, one where the
first argument is bottom, and one where the second one is bottom. All
other values that are not the same \hs{Tri} and not bottom go to
\hs{False}.

If there for models of this function which includes
another value $\diamond$, other values than those of this data type and
bottom, must have $\fn{equal}(\diamond,x) = \fn{false}$, for all non
bottom $x$, including $\diamond$. This is not a problem per se, since the
Haskell source is well typed, and \hs{equal} will not be applied to
anything but \hs{Tri}s and bottoms.

One weakness of that approach is that two functions that are
extensionally equivalent in Haskell are not in the generated theory,
when applied to non sense arguments. An example of this behaviour is
these two implementations of the boolean and function in Figure
\ref{code:and}.

\begin{figure}[h!]
\centering
\begin{minipage}[b]{6cm}
\begin{code}
and :: Bool -> Bool -> Bool
and True  b = b
and False _ = False
\end{code}
\end{minipage}
\hspace{10pt}
\begin{minipage}[b]{6cm}
\begin{code}
and' :: Bool -> Bool -> Bool
and' True  b = b
and' _     _ = False
\end{code}
\end{minipage}
\caption{Two definitions of boolean and, \texttt{and} and \texttt{and'}
\label{code:and}
}
\end{figure}

Let us analyse the models of this program in Figure \ref{code:and}
with an additional value $\diamond$ in the domain. The translation
makes $\fn{and}(\diamond,\fn{true}) = \bot$, since there is a wild
pattern that goes to $\bot$. For the other function we have
$\fn{and'}(\diamond,\fn{true}) = \fn{false}$, as the wild pattern
already goes to $\fn{false}$. Because of this differences, the
approximation lemma cannot prove their extensional equality. While
this is a simple example, it is easy to imagine more complex cases
where different pattern matching techniques makes no difference to the
Haskell program, but to this translation.

This suggests that wild patterns should be expanded to pattern for all
their constructors, and add a match any pattern that goes to
bottom. This make the the meta theorem
$\faa{x}{y} \fn{and}(x,y) = \fn{and'}(x,y)$ consistent with rest of the
theory.  Further, it would be provable by the approximation lemma, and
in more complex cases with recursive functions, by fixed point
induction. This translation is also assumed to be a little easier to
implement. The down side is that functions such as \hs{equal} above
would generate $O(n^2)$ axioms, where $n$ is the number of constructors for
the data type. The presence of GADTs and other type extensions such as
type families would requite type information and add a lot of complexity.

\section{Lemmas}

For many properties, especially more advanced ones, it is crucial to
be able to use lemmas to obtain a proof. The introduced proof
techniques in this theses are just not strong enough. For structural
induction, it is possible to do induction in more than one depth. This
sometimes make the hypotheses equally strong as the required lemmas,
as in the example of plus commutativity of natural numbers in Section
\ref{sec:genind}. But this approach fails for properties about
multiplication: it is essential to use lemmas about addition.

The concept of adding lemmas is of course quite simple. Assume your
program has two properties \hs{prop\_a} and \hs{prop\_b}, and the
second needs the first as a lemma. If \hs{prop\_a} succeeds, then the
should program just add that property to the theory generated for
\hs{prop\_b}, and try it again. Unfortunately, adding a property to a
theory needs to be carried out with care. For properties that hold for
infinite and partial values it is actually a bit simpler than for
those that are only true for finite values. These settings are
addressed one by one below.

\subsection{Lemmas from Theorems}
\label{sec:thmlemmas}

One example of a Theorem, a property that holds for partial and
infinite values, is the right identity of \hs{||}, boolean or:

$$\fa{x} x \w \hs{||} \w \fn{false} = x$$

\noindent
Adding this meta theorem to the theory would make it
inconsistent. Again, in a model with an extra value $\diamond$, we have
that $\diamond \w \hs{||} \w \fn{false} = \bot$. However, we can create a
function that forces a value to be a \hs{Bool} or $\bot$ as this:

\begin{align*}
\rom{1} \qquad &&        & \fn{force}(\fn{true})  && = \fn{true} \\
\rom{2} \qquad &&        & \fn{force}(\fn{false}) && = \fn{false} \\
\rom{3} \qquad && \fa{x} & \fn{force}(x)          && = \bot \vee x = \fn{false} \vee x = \fn{true}
\end{align*}

The theory would be consistent with this reformulation of the meta theorem:

$$\fa{x} \fn{force}(x) \w \hs{||} \w \fn{false} = \fn{force}(x)$$

It is easy to generalise $\fn{force}$ beyond booleans. Make it a
binary function with first argument a description of the type. Each
simply kinded type would be given a constant, and higher-kinded types
functions, an example would be $\fn{list}(\alpha)$ for lists of $\alpha$:

\begin{align*}
\rom{1} \qquad && \fa{\alpha}          & \fn{force}(\fn{list}(\alpha),\fn{nil})        && = \fn{nil} \\
\rom{2} \qquad && \faaa{\alpha}{x}{xs} & \fn{force}(\fn{list}(\alpha),\fn{cons}(x,xs)) && = \fn{cons}(\fn{force}(\alpha,x),\fn{force}(\fn{list}(\alpha),xs) \\
\rom{3} \qquad && \faa{\alpha}{xs}     & \fn{force}(\fn{list}(\alpha),xs)              && = \bot  \\
               &&                      & \multicolumn{3}{l}{\vee xs = \fn{nil} \vee xs = \fn{cons}(\fn{cons_0}(xs),\fn{cons_1}(xs))}
\end{align*}

Using functions and predicates effectively to witness type information
has been studied by \cite{sortMonotonicity} and by
\cite{polyMonotonicity}.

Proofs by definitional equality would also benefit from type tags. The
the current limitations are discussed section
\ref{sec:concreteconcerns}. Furthermore, the extensional equality of
the different implementations of the or function in Figure
\ref{code:and} is not provable with approximation lemma with the
current translation. However, stated with $\fn{force}$-tags it should
be possible. This could give us the best of two worlds: minimal
translation of pattern matching and wider applicability of untyped
proof methods.

\subsection{Lemmas from Finite Theorems}
\label{sec:total}

\newcommand\Inf{\fn{Inf}}
\newcommand\Total{\fn{Total}}
\newcommand\Fin{\fn{Fin}}
The $\fn{force}$ function from the previous section is not easily
generatised to remove partiality from a value. Another way to do this
is to introduce a predicate $\Total$, indicating totality. This is a
way to axiomatise $\Total$ for natural numbers:

\begin{align*}
\rom{1} &&        & \neg \, \Total(\bot) \\
\rom{2} &&        & \Total(\fn{zero}) \\
\rom{3} && \fa{x} & \Total(x) \rightarrow \Total(\fn{succ}(x))
\end{align*}

As with the $\fn{force}$ function, it could be wise to add an argument
to $\Total$ indicating the type, but for the rest of this section we
will only consider the (Haskell) natural numbers. A property such as the
commutativity of plus could be expressed with this meta theorem:

\begin{equation*}
\faa{x}{y} \Total(x) \wedge \Total(y) \rightarrow x + y = y + x
\end{equation*}

We would also like to express infiniteness. Our canonical example of a
an infinite value is the natural number \hs{inf = Succ inf}. This is
the only infinite natural number so a prediate $\Inf$ for infinite
natural numbers is easy to axiomatise:

\begin{equation*}
\fa{x} \Inf(x) \leftrightarrow (x = \fn{succ}(x))
\end{equation*}

While defining $\Inf$ is straightforward for natural numbers, further
complexities are added for other data types and introduction schemas
are needed. But since Haskell functions are continuous and finite,
every infinite value is produced from finite information (\hs{repeat},
\hs{iterate}, \hs{enumFrom}), so a predicate $\Inf$ should be enough
for our purposes, and $\Inf$ should be defined so that it excludes
partial values, that is satisfying $\Inf(x) \rightarrow \Total(x)$.

\pagebreak

We can now define finiteness of a value $x$ as
$\Fin(x) \leftrightarrow \Total(x) \wedge \neg \Inf(x)$.  The results
from a function would be necessary to axiomatise to make such axioms
useable. To exemplify why this is not trivial, consider the
complexities of the plus function:

\begin{align*}
\rom{1} && \faa{x}{y} & \Fin(x) \wedge \Fin(y)                  & \leftrightarrow &&& \Fin(x + y) \\
\rom{2} && \faa{x}{y} & \Inf(x) \vee (\Fin(x) \wedge \Inf(y)) & \leftrightarrow &&& \Inf(x + y)
\end{align*}

\noindent
Axiom $\romnodot{1}$ asserts that $x + y$ is finite iff $x$ and $y$
is. Further, $x + y$ is infinite iff then $x$ is or $x$ is finite and
$y$ is infinite. This is expressed in Axiom $\romnodot{2}$. How to
prove such statuses for the return value of a function in terms of
$\Fin$, $\Total$ and $\Inf$ for different statuses of arguments is an
open problem. Possible sources for inspiration is the work by
\cite{exhaustiblesets}, where topological compactness for data types
play a significant r\^{o}le.

\subsection{Speculating Lemmas}

The previous sections assumed that the necessary lemmas are present as
properties in the source file. This puts a lot of burden on the
programmer to both figure out required lemmas and to state them, it is
not always clear which lemmas are required to prove a given
property. This section discusses some ideas to generate candidate
lemmas.

Inductive proofs by rippling enables techinques such as cricics
\citep{productiveuse}, which makes lemmas and generalizations when
rippling fails. As our approach relies on theorem provers, it is very
hard to extract some information from a timeout of a proof by induction.

Another approach is to do property speculation, implemented for
Isabell \citep{isacosy} and Haskell \citep{quickspec}. The idea is to
use the signatures from the functions in the source file and it try to
find equality properties by creating small syntax trees of the
functions by testing. Such equalities are well suited as lemmas for
more complex properties, and it would be very interesting to extend
our approach with a lemma synthesis.


\section{Type Classes}
\label{sec:typeclasses}

There are some obstacles to support type classes. One is introduced by
the technicalities of type inference to decide instances in the
program. Another is how to express type classes in logic. An approach
would be to use dictionary passing, inlined for concrete types.

A third obstacle is to decide which axioms to use for the functions
from a type class, as there is no set agreement on how strong the
rules are that we expect from some instances to obey. An example
is the Eq class, whose instances typically only constitute an
equivalence relation for finite values.

\section{Quantifying over Functions and Free Theorems}

Polymorphic functions generally enjoy free theorems
\citep{freetheorems} which could be added to the theory.  Furthermore,
the functions quantified over are monotone (and continuous), and this
is currently not enforced in the generated theories. An open question
is how this effects the results. As seen in Section
\ref{sec:defeqandapprox}, definitional equality would be stronger if
types of the result of a function was expressed.


\section{Material Implication and Existential Quantifiers}

To be able to prove more complex properties we would like to be able
to prove properties with implications and existential quantification.
This example about soundness for a prefix predicate is given by
\cite{smallcheck}:

\begin{code}
prop_isPrefixSound :: Eq a => [a] -> [a] -> Prop (Bool :=> Exists [a] Bool)
prop_isPrefixSound xs ys = isPrefix xs ys ==> exists (\xs' -> xs ++ xs' == ys)
\end{code}

Finite structural induction could be extended to prove such a
property. However, implication is not an admissible property which
means that other coinductive proof techniques are not
applicable. Existential quantification over functions would require a
set of function combinators in the theory to construct a witness.

\section{Other Proof Techniques}

Each of the proof methods have their own future work chapter, but
there are other techniques not implemented. One is recursion induction
\citep{recind}, where you prove that two functions are equal by
asserting that one of them fulfills the same equations as the
other. Another is bisimulation \citep{bisimulationCapretta}, which
could possibly be extended to prove properties about total but
infinite values. To prove properties about infinite streams, an
automated approach using anamorphisms and idiom laws as in
\cite{streams} could be used.

\section{Faster Proof Searches via Predicates}
\label{sec:minpredicate}

It is possible to add annotations in the equations generated for
functions to make the theorem prover not unroll unnecessary
definitions and regard these equalities more like definitions. This
would avoid the theorem provers to get ``lost'' in the search space,
and in some cases also allow finite models. The predicate indicates
that an expression is suitable of reducing to weak head normal
form. This predicate would be need to be added at various places in
the translated theory, an example is that functions doing pattern
matching would need to indicate that the expression under scrutiny is
suitable for reduction.
