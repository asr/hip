\chapter{Future work}
\label{ch:future}

The chapter addresses some limitations of our approach and ideas how
to get around them.

\section{Pattern Matching Re-Revisited}
\label{sec:rerevisited}

The current translation of pattern matching is described in Section
\ref{sec:patternsrevisited}. One of the strengths of it is the
translation of wild patterns. Consider this function for equality of a
data type with three elements:

\begin{code}
data Tri = A | B | C

equal :: Tri -> Tri -> Bool
equal A A = True
equal B B = True
equal C C = True
equal _ _ = False
\end{code}

\noindent
The translation adds two more cases that go to bottom, one where the
first argument is bottom, and one where the second one is bottom. All
other values that are not the same \hs{Tri} and not bottom go to
\hs{False}.

If there for models of this function which includes
another value $\diamond$, other values than those of this data type and
bottom, must have $\fn{equal}(\diamond,x) = \fn{false}$, for all non
bottom $x$, including $\diamond$. This is not a problem per se, since the
Haskell source is well typed, and \hs{equal} will not be applied to
anything but \hs{Tri}s and bottoms.

One weakness of that approach is that two functions that are
extensionally equivalent in Haskell are not in the generated theory,
when applied to non sense arguments. An example of this behaviour is
these two implementations of the boolean or function in Figure
\ref{code:or}.

\begin{figure}[h!]
\centering
\begin{minipage}[b]{6cm}
\begin{code}
or :: Bool -> Bool -> Bool
or False b = b
or True  _ = True
\end{code}
\end{minipage}
\hspace{10pt}
\begin{minipage}[b]{6cm}
\begin{code}
or' :: Bool -> Bool -> Bool
or' False b = b
or' _     _ = True
\end{code}
\end{minipage}
\caption{Two definitions of boolean and, \texttt{and} and \texttt{and'}
\label{code:or}
}
\end{figure}

Let us analyse the models of this program in Figure \ref{code:or} with
an additional value $\diamond$ in the domain. The translation makes
$\fn{or}(\diamond,\fn{true}) = \bot$, since there is a wild pattern that
goes to $\bot$. For the other function we have
$\fn{or'}(\diamond,\fn{true}) = \fn{false}$, as the wild pattern already
goes to $\fn{false}$. Because of this differences, the approximation
lemma cannot prove this property. While this is a simple example, it
is easy to imagine more complex cases where different pattern matching
techniques makes no difference to the Haskell program, but to this
translation.

This suggests that wild patterns should be expanded to pattern for all
their constructors, and add a match any pattern that goes to
bottom. This make the the meta theorem
$\faa{x}{y} \fn{or}(x,y) = \fn{or'}(x,y)$ consistent with rest of the
theory.  Further, it would be provable by the approximation lemma, and
in more complex cases with recursive functions, by fixed point
induction. This translation is also assumed to be a little easier to
implement. The down side is that functions such as \hs{equal} above
would generate $O(n^2)$ axioms, where $n$ is the number of constructors for
the data type. The presence of GADTs and other type extensions such as
type families would requite type information and add a lot of complexity.

\section{Lemmas}

For many properties, especially more advanced ones, it is crucial to
be able to use lemmas to obtain a proof. The introduced proof
techniques in this theses are just not strong enough. For structural
induction, it is possible to do induction in more than one depth. This
sometimes make the hypotheses equally strong as the required lemmas,
as in the example of plus commutativity of natural numbers in Section
\ref{sec:genind}. But this approach fails for properties about
multiplication: it is essential to use lemmas about addition.

The concept of adding lemmas is of course quite simple. Assume your
program has two properties \hs{prop\_a} and \hs{prop\_b}, and the
second needs the first as a lemma. If \hs{prop\_a} succeeds, then the
should program just add that property to the theory generated for
\hs{prop\_b}, and try it again. Unfortunately, adding a property to a
theory needs to be carried out with care. For properties that hold for
infinite and partial values it is actually a bit simpler than for
those that are only true for finite values. These settings are
addressed one by one below.

\subsection{Lemmas from Theorems}

One example of a Theorem, a property that holds for partial and
infinite values, is the right identity of \hs{||}, boolean or:

$$\fa{x} x \w \hs{||} \w \fn{false} = x$$

\noindent
Adding this meta theorem to the theory would make it
inconsistent. Again, in a model with an extra value $\diamond$, we have
that $\diamond \w \hs{||} \w \fn{false} = \bot$. However, we can create a
function that forces a value to be a \hs{Bool} or $\bot$ as this:

\begin{align*}
\rom{1} \qquad &&        & \fn{force}(\fn{true})  && = \fn{true} \\
\rom{2} \qquad &&        & \fn{force}(\fn{false}) && = \fn{false} \\
\rom{3} \qquad && \fa{x} & \fn{force}(x)          && = \bot \vee x = \fn{false} \vee x = \fn{true}
\end{align*}

The theory would be consistent with this reformulation of the meta theorem:

$$\fa{x} \fn{force}(x) \w \hs{||} \w \fn{false} = \fn{force}(x)$$

It is easy to generalise $\fn{force}$ beyond booleans. Make it a
binary function with first argument a description of the type. Each
simply kinded type would be given a constant, and higher-kinded types
functions, an example would be $\fn{list}(\alpha)$ for lists of $\alpha$:

\begin{align*}
\rom{1} \qquad && \fa{\alpha}          & \fn{force}(\fn{list}(\alpha),\fn{nil})        && = \fn{nil} \\
\rom{2} \qquad && \faaa{\alpha}{x}{xs} & \fn{force}(\fn{list}(\alpha),\fn{cons}(x,xs)) && = \fn{cons}(\fn{force}(\alpha,x),\fn{force}(\fn{list}(\alpha),xs) \\
\rom{3} \qquad && \faa{\alpha}{xs}     & \fn{force}(\fn{list}(\alpha),xs)              && = \bot  \\
               &&                      & \multicolumn{3}{l}{\vee xs = \fn{nil} \vee xs = \fn{cons}(\fn{cons_0}(xs),\fn{cons_1}(xs))}
\end{align*}

Using functions and predicates effectively to witness type information
has been studied by \cite{sortMonotonicity} and by
\cite{polyMonotonicity}.

Proofs by definitional equality would also benefit from type tags. The
the current limitations are discussed section
\ref{sec:concreteconcerns}. Furthermore, the extensional equality of
the different implementations of the or function in Figure
\ref{code:or} is not provable with approximation lemma with the
current translation. However, stated with $\fn{force}$-tags it should
be possible. This could give us the best of two worlds: minimal
translation of pattern matching and wider applicability of untyped
proof methods.

\subsection{Lemmas from Finite Theorems}

The $\fn{force}$ function from the previous section is not easily
generatised to remove partiality from a value. Another way to do this
is to introduce a predicate $Total$, indicating totality. This is a
way to axiomatise $Total$ for natural numbers:

\begin{align*}
\rom{1} &&        & \neg \, Total(\bot) \\
\rom{2} &&        & Total(\fn{zero}) \\
\rom{3} && \fa{x} & Total(x) \rightarrow Total(\fn{succ}(x))
\end{align*}

As with the $\fn{force}$ function, it could be wise to add an argument
to $Total$ indicating the type, but for the rest of this section we
will only consider the (Haskell) natural numbers. A property such as the
commutativity of plus could be expressed with this meta theorem:

\begin{equation*}
\faa{x}{y} Total(x) \wedge Total(y) \rightarrow x + y = y + x
\end{equation*}

We would also like to express finiteness. Our canonical example of a
an infinite value is the natural number
\hs{inf = Succ inf}. Identification of such cyclic values can be axiomatised:

\begin{equation*}
\fa{x} Cyclic(x) \leftrightarrow (x = \fn{succ}(x) \vee (\ex{y} x = \fn{succ}(y) \wedge Cyclic(y))) \\
\end{equation*}

Because Haskell functions are continuous and finite, every infinite
value is produced from finite information (\hs{repeat}, \hs{iterate},
\hs{enumFrom}), so a predicate $Cyclic$ should be enough for our
purposes. Further, it is clear that $Cyclic(x) \rightarrow Total(x)$.

\pagebreak

We can now define finiteness of a value $x$ as
$Finite(x) \leftrightarrow Total(x) \wedge \neg Cyclic(x)$.  The results
from a function would be necessary to axiomatise to make such axioms
useable. To exemplify why this is not trivial, consider the
complexities of the plus function:

\begin{align*}
\rom{1} && \faa{x}{y} & Finite(x) \wedge Finite(y)                  & \leftrightarrow &&& Finite(x + y) \\
\rom{2} && \faa{x}{y} & Cyclic(x) \vee (Finite(x) \wedge Cyclic(y)) & \leftrightarrow &&& Cyclic(x + y)
\end{align*}

How to prove the status for the return value of a function in terms of
$Finite$, $Total$ and $Cyclic$ for different statuses of the arguments is
an open problem. Possible sources for inspiration is the work by
\cite{exhaustiblesets}, where topological compactness for
data types play a significant r\^{o}le.

\subsection{Speculating Lemmas}

The previous sections assumed that the necessary lemmas are present as
properties in the source file. This puts a lot of burden on the
programmer to both figure out required lemmas and to state them, it is
not always clear which lemmas are required to prove a given
property. This section discusses some ideas to generate candidate
lemmas.

Inductive proofs by rippling enables techinques such as cricics
\citep{productiveuse}, which makes lemmas and generalizations when
rippling fails. As our approach relies on theorem provers, it is very
hard to extract some information from a timeout of a proof by induction.

Another approach is to do property speculation, implemented for
Isabell \cite{isacosy} and Haskell \cite{quickspec}. The idea is to
use the signatures from the functions in the source file and it try to
find equality properties by creating small syntax trees of the
functions by testing. Such equalities are well suited as lemmas for
more complex properties, and it would be very interesting to extend
our approach with a lemma synthesis.


\section{Type classes}
\label{sec:typeclasses}

There are some obstacles to support type classes. One is introduced by
the technicalities of type inference to decide instances in the
program. Another is how to express type classes in logic. An approach
would be to use dictionary passing, inlined for concrete types.

A third obstacle is to decide which axioms to use for the functions
from a type class. Let us take Monoids as an example. Its signature
consists of a binary operator and an element from the type. The
required laws are associativity of the operator and that the element
is the identity element for this operator. Lists are an instance of
Monoid, and the associativity law holds for finite and partial lists,
but the right identity does not hold: \hs{$\bot$ ++ []} is not equal
to \hs{[]}, as it is equal to $\bot$. This example suggests that it
could be appropriate to assume the laws only for total values, and
that it would be sensible to automatically check these laws for all
instances.

We know something else about the functions from a type class: they are
all continuous. This is also true for functions which are quantified
over. This is currently not enforced in the generated theories, and an
open question is how this effects the results.

\section{Material Implication and Existential Quantifiers}

To be able to prove more complex properties we would like to be able
to prove properties with implications and existential quantification.
This example about soundness for a prefix predicate is given by
\cite{smallcheck}:

\begin{code}
prop_isPrefixSound :: Eq a => [a] -> [a] -> Prop (Bool :=> Exists [a] [a])
prop_isPrefixSound xs ys = isPrefix xs ys ==> exists (\xs' -> xs ++ xs' == ys)
\end{code}

Finite structural induction could be extended to prove such a
property. However, implication is not an admissible property which
means that other coinductive proof techniques are not
applicable. Existential quantification over functions would require a
set of function combinators in the theory to construct a witness.

\section{Other Proof Techniques}

Each of the proof methods have their own future work chapter, but
there are of other techniques not implemented. One is recursion
induction \cite{recind}, where you prove that two functions are equal
by asserting that one of them fulfills the same equations as the
other. Another is bisimulation \cite{bisimulationCapretta}, which
could possibly be extended to prove properties about total but
infinite values.

\section{Faster Proof Searches via Predicates}

It is possible to add annotations in the equations generated for
functions to make the theorem prover not unroll unnecessary
definitions and regard these equalities more like definitions. This
would avoid the theorem provers to get ``lost'' in the search space,
and in some cases also allow finite models.
