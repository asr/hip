\chapter{Technical Part}

Implementation technicalities.

% Translation to FOL ----------------------------------------------------------

\input{translation}

% End of Translation to FOL ---------------------------------------------------

\section{Proof techniques}

\note{Need to decide some running example}

To prove things using this technique, properties are entered in the
Haskell source code. A small prelude called \hs{AutoPrelude} needs to
be imported that gives access to the relevant functions. One example
is the associativity of list concatenation:

\begin{verbatim}
import AutoPrelude

prop_app_assoc :: [a] -> [a] -> [a] -> Prop [a]
prop_app_assoc xs ys zs = xs ++ (ys ++ zs) =:= (xs ++ ys) ++ zs
\end{verbatim}

The infix function \hs{=:=} comes from the imported library, as well
as the type constructor \hs{Prop}. The type signature cannot be
omitted as this is used for instance to see which kind of induction we
will use.

Running the program is simple. Just save the file as for instance
\hs{ListProps.hs} and run

\begin{verbatim}
autospec ListProps.hs
\end{verbatim}

and the program will report if it was provable or not, and which
techniques succeeded.

These properties are also QuickCheck-testable, so you can run the
normal \hs{quickCheck} function on them, given that there are
relevant \hs{Eq} and \hs{Arbitrary} instances provided.

Let's now take a look what different proof methods that are supported.
\note{This is too informal}

\subsection{Definitional Equality}

Some properties cannot or need not use induction or some more
sophisticated technique, since they are true by definition. Examples
are properties for fully polymorphic functions such as the definition
of \hs{id} in the SK-calculus, here

\begin{verbatim}
s f g x = f x (g x)
k x y = x
id x = x

prop_skk_id :: Prop (a -> a)
prop_skk_id = s k k =:= id
\end{verbatim}

Then, the generated conjecture is simply

\begin{equation*}
\app{ (\app {\ptr{s}} {\ptr{k}} )
    }{\ptr{k}}} = \ptr{id}
\end{equation*}

Another example where this is useful is to prove functor and monad
laws for the environment monad.

\subsubsection{Extensional Equality and seq}

To prove the previous property we also need to have extensional
equality, postulated with the following axiom

\begin{equation*}
\faa{f}{g} (\fa{x} \app{f}{x} = \app{g}{x}) \rightarrow f = g
\end{equation*}

which identifies function pointers and functions composed with $@$.
One problem with extensional equality in Haskell, is that the presence
of \hs{seq} breaks it. \hs{seq} is a built in function with the
following behavior:

\begin{verbatim}
seq :: a -> b -> b
seq bottom b = bottom
seq a      b = b
\end{verbatim}

Where \hs{a} is any other value than $\bot$. With \hs{seq} it is
possibly to distinguish between these two functions, which otherwise
are observationally equal:

\begin{verbatim}
f = bottom
g = \x -> bottom
\end{verbatim}

Because \hs{seq f ()} evaluates to $\bot$, and \hs{seq g ()} evaluates
to \hs{()}, but on any argument \hs{f} and \hs{g} gets, they both
return $\bot$. Here we also need an extra axiom, which says that
anything applied to $\bot$ is $\bot$:

\begin{equation*}
\fa{x} \app{\bot}{x} = \bot
\end{equation*}

However, \hs{seq} is the only function that can do this, so we will
ignore its presence in Haskell.
\note{This could be added as a switch \hs{--enable-seq}, which removes
  extensional equality}

Furthermore, if we assume we have extensional equality we also have
the property that \hs{Prop (a -> b)} is equivalent to
\hs{a -> Prop b}, by letting the property have an extra argument that
is applied to the left and right hand side of the equality. This has
two benefits, firstly it can trigger other proof methods should \hs{a}
or \hs{b} be concrete types (the former for induction and the latter
for approximation lemma), and secondly it does not need to use the
extensionality axiom introduced above which is costly and confusing
for the theorem provers tested.
\note{Add some support for this claim. For instance, SPASS seems to be
  utterly horrible at this, but Eprover and Vampire are OK}

\subsubsection{Concrete Concerns}

This only works on non-concrete types because of the way bottoms are
added. One example when this is a problem with is this plausible
definition of boolean or

\begin{verbatim}
False || a = a
True  || _ = True
\end{verbatim}

\note{This blends a bit with future work. One solution is just to add
  a predicate describing the elements of the type. Hopefully I will have time
  to implement this so it need not be in future work. Further, it is
  pretty sloppily written}
Then an extra branch is added that matches everything else that goes
to $\bot$. This is of efficiency reasons: imagine if we had only used
just a few constructors of a data type with hundreds of constructors,
then we do not want to write a new line with all those constructors to
$\bot$. A property that should indeed hold (regardless of presence of
partial values or not) is \hs{x || False == x} for all
\hs{x}. However, for models with another point, say $\top$, we get
that $\top \, \fn{||} \, \fn{False} = \bot$, and the conjecture is
counter satisfiable.


\subsection{Structural induction}

Any non-recursive, or more importantly recursive data type gives rise
to induction schemata.

\footnote{Haskell's natural numbers are of course also cluttered with
  elements that are not natural numbers, such as $\bot$, but also the
  infinite ``natural number'' defined by \hs{infinite = Succ infinite}}
, defined the usual way in Haskell by \hs{data Nat = Zero | Succ Nat}
yields this induction axiom schema:

\begin{mathpar}
  \inferrule*
     {
       P(\fn{Zero})
       \\
       \fa{x} P(x) \rightarrow P(\fn{Succ}(x))
     }
     { \fa{x} P(x) }
\end{mathpar}

\subsection{Fixpoint induction}

\note{fixpoint induction or fixed point induction?}
\note{Introduce \hs{fix} in background, and reference them to each
  other?}
The celebrated fixpoint induction from Domain Theory, now automated!

\begin{mathpar}
  \inferrule*
     {
       P(\bot)
       \\
       P(x) \rightarrow P(f(x))
       \\
       P \, \mathrm{admissible}
     }
     { P(\fn{fix} f) }
\end{mathpar}

Here it is important that $P$ is \emph{admissible} or
$\omega$-\emph{complete}, but universally quantified equalities of
continuous functions and compositions thereof is.

A simple way of defining a fixed point combinator in Haskell is
\begin{verbatim}
fix :: (a -> a) -> a
fix f = f (fix f)
\end{verbatim}

The translation from a function written with explicit recursion to a
style with \hs{fix} is mechanical. Consider \hs{iterate}

\begin{verbatim}
iterate f x = x : iterate f (f x)
\end{verbatim}

Each occurrence of the same function is copied with a new definition,
here \hs{iterateStep}, with an additional argument (here \hs{i}) which
replaces the recursive call.

\begin{verbatim}
iterate = fix iterateStep
iterateStep i f x = x : i f (f x)
\end{verbatim}

Now,
\hs{fix iterateStep} =
\hs{iterateStep (fix iterateStep}) =
\hs{iterateStep (iterateStep (fix iterateStep))} = ...
\hs{iterateStep (iterateStep (... (iterateStep (fix iterateStep))
  ...))}, which allows arbitrary unrolling of the definition, making it
  equivalent to \hs{iterate} above.

\subsubsection{Implementation}

To prove something regarding a function $f$ defined abstractly with
arguments $\overbar{x}$ and body $e(\overbar{x},f)$ as

\begin{equation*}
f \, \overbar{x} = e(\overbar{x},f)
\end{equation*}



We introduce two new constants, $\tofix{f}$ and $\unfix{f}$ as this:

\begin{equation*}
\tofix{f} \, \overbar{x} = e(\overbar{x},\unfix{f})
\end{equation*}

\note{Some side-by side reasoning how it looks with these definitions
  and the normal definitions of fixpoint induction}
The fixpoint schema now becomes:

\begin{mathpar}
  \inferrule*
     {
       P(\bot)
       \\
       P(\unfix{f}) \rightarrow P(\tofix{f})
       \\
       P \, \mathrm{admissible}
     }
     { P(f) }
\end{mathpar}


This also works for several functions at the same time, possibly
mutually recursive:

\begin{mathpar}
  \inferrule*
     {
       P(\bot,\bot)
       \\
       P(\unfix{f},\unfix{g}) \rightarrow P(\tofix{f},\tofix{g})
       \\
       P \, \mathrm{admissible}
     }
     { P(f,g) }
\end{mathpar}

\subsubsection{Natural Selection}

Given a property like list concatenation as above
\note{examples are not set in stone so this may change}
it is actually not possible to show the property with fixed point
induction on all four occurrences of \hs{++}. As if you would do
structural induction on this proof, you will see that the both in the
left hand side are normalized, and the first on the right hand
side. These are the functions you will want to do fixpoint induction
on. See illustration with $\mathbf{app}$ on the locations suspect for
fixpoint induction and $app$ for no change:

\begin{equation*}
\faaa{xs}{ys}{zs} \mathbf{app}(\mathbf{app}(xs,ys),zs)
               =  \mathbf{app}(xs,app(ys,zs))
\end{equation*}
\note{I cannot prove the \mathbf{[]} case for this}

The approach taken here is to try fixed point induction on all the
subsets of the recursive functions mentioned in the property.

\subsection{Approximation lemma}

After fixed point induction since it is an easy consequence of fixed
point induction, and how we removed the auxiliary structure of natural
numbers. This makes it equivalent to fixed point induction of id on
both sides (or does it?)

Approximation lemma is a generalization of the take lemma, and its
first form is used for properties about infinite and partial lists,
but it is easily generalized to other recursive data types.
\note{
In particular, all polynomial datatypes — for example, any
sum-of-products datatype — can be defined in this way.
This result generalises
to mutually recursive, parameterised, exponential and nested datatypes, but
for simplicity we only consider polynomial datatypes in this article.
\cite{genapprox}}

